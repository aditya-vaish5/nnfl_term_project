{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of nnfl_termproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-vaish5/nnfl_term_project/blob/master/Copy_of_nnfl_termproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uer1xgJvH6L6",
        "colab_type": "code",
        "outputId": "b3a22a9f-a9da-4867-a9cd-be8c352919cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-25 14:15:51--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13603614 (13M) [text/plain]\n",
            "Saving to: ‘train.en.1’\n",
            "\n",
            "train.en.1          100%[===================>]  12.97M  7.49MB/s    in 1.7s    \n",
            "\n",
            "2020-05-25 14:15:53 (7.49 MB/s) - ‘train.en.1’ saved [13603614/13603614]\n",
            "\n",
            "--2020-05-25 14:15:55--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18074646 (17M) [text/plain]\n",
            "Saving to: ‘train.vi.1’\n",
            "\n",
            "train.vi.1          100%[===================>]  17.24M  9.92MB/s    in 1.7s    \n",
            "\n",
            "2020-05-25 14:15:57 (9.92 MB/s) - ‘train.vi.1’ saved [18074646/18074646]\n",
            "\n",
            "--2020-05-25 14:16:00--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 132264 (129K) [text/plain]\n",
            "Saving to: ‘tst2013.en.1’\n",
            "\n",
            "tst2013.en.1        100%[===================>] 129.16K   411KB/s    in 0.3s    \n",
            "\n",
            "2020-05-25 14:16:01 (411 KB/s) - ‘tst2013.en.1’ saved [132264/132264]\n",
            "\n",
            "--2020-05-25 14:16:04--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 183855 (180K) [text/plain]\n",
            "Saving to: ‘tst2013.vi.1’\n",
            "\n",
            "tst2013.vi.1        100%[===================>] 179.55K   471KB/s    in 0.4s    \n",
            "\n",
            "2020-05-25 14:16:05 (471 KB/s) - ‘tst2013.vi.1’ saved [183855/183855]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advDQCZZIi60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlaQsvAUO9Jm",
        "colab_type": "code",
        "outputId": "572e6a33-ba76-4a85-bc55-572aa317a346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "source_sent = []\n",
        "target_sent = []\n",
        "\n",
        "test_source_sent = []\n",
        "test_target_sent = []\n",
        "\n",
        "\n",
        "with open('train.en', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        # discarding first 20 translations as there was some\n",
        "        # english to english translations found in the first few. which are wrong\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        source_sent.append(line)\n",
        "        \n",
        "            \n",
        "with open('train.vi', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        target_sent.append(line)\n",
        "\n",
        "\n",
        "with open('tst2013.en', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        test_source_sent.append(line)\n",
        "                    \n",
        "with open('tst2013.vi', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        test_target_sent.append(line)\n",
        "            \n",
        "assert len(source_sent) == len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))\n",
        "assert len(test_source_sent) == len(test_target_sent), 'Source: %d, Target: %d'%(len(test_source_sent), len(test_target_sent))\n",
        "print('Sample translations (%d)'%len(source_sent))\n",
        "for i in range(0,len(source_sent),10000):\n",
        "    print('(',i,') EN: ', source_sent[i])\n",
        "    print('(',i,') VI: ', target_sent[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample translations (133267)\n",
            "( 0 ) EN:  In each one of those assessments that we write , we always tag on a summary , and the summary is written for a non-scientific audience .\n",
            "\n",
            "( 0 ) VI:  Trong mỗi bản đánh giá chúng tôi viết , chúng tôi luôn đính kèm một bản tóm lược , được viết cho những độc giả không chuyên về khoa học .\n",
            "\n",
            "( 10000 ) EN:  This is an area in the prefrontal cortex , a region where we can use cognition to try to overcome aversive emotional states .\n",
            "\n",
            "( 10000 ) VI:  Đây là một khu vực trong vỏ não trước trán , vùng mà chúng sử dụng tri thức cho việc thử vượt qua trạng thái cảm xúc ác cảm .\n",
            "\n",
            "( 20000 ) EN:  And there are flowers that are self-infertile . That means they can &apos;t -- the pollen in their bloom can &apos;t fertilize themselves .\n",
            "\n",
            "( 20000 ) VI:  có những loài hoa không thể tự thụ phấn . Nghĩa là chúng không thể -- phấn hoa của nó không thể tụ thụ phấn được\n",
            "\n",
            "( 30000 ) EN:  And a lot of this comes together in a philosophy of change that I find really is powerful .\n",
            "\n",
            "( 30000 ) VI:  Và nhiều như vậy hợp lại thành một triết lý của sự thay đổi mà tôi thấy là thực sự rất mạnh .\n",
            "\n",
            "( 40000 ) EN:  Dean Ornish : At first for a long time , I wrote messages in notebooks .\n",
            "\n",
            "( 40000 ) VI:  Dean Ornish : &quot; Trong một khoảng thời gian dài ban đầu , tôi đã viết các tin nhắn trên các cuốn ghi chú .\n",
            "\n",
            "( 50000 ) EN:  World &apos;s first bamboo bike with folding handlebars .\n",
            "\n",
            "( 50000 ) VI:  Chiếc xe đạp bằng tre đầu tiên trên thế giới với ghi đông gập .\n",
            "\n",
            "( 60000 ) EN:  We need to invest more resources into research and treatment of mental illness .\n",
            "\n",
            "( 60000 ) VI:  Chúng ta cần đầu tư nhiều nguồn lực hơn cho công cuộc nghiên cứu và chữa trị về bệnh thần kinh .\n",
            "\n",
            "( 70000 ) EN:  If we are providing knowledge and experience , we need to structure that .\n",
            "\n",
            "( 70000 ) VI:  Nếu chúng ta cung cấp kiến thức và kinh nghiệm , chúng ta cần cơ cấu nó .\n",
            "\n",
            "( 80000 ) EN:  But I say it has to be under the conditions I &apos;ve always worked : no credit , no logos , no sponsoring .\n",
            "\n",
            "( 80000 ) VI:  Nhưng tôi nói nó phải theo các điều kiện tôi luôn luôn làm không có tín dụng , không có biểu tượng , không có tài trợ .\n",
            "\n",
            "( 90000 ) EN:  What would it look like ?\n",
            "\n",
            "( 90000 ) VI:  Nó sẽ trông như thế nào ?\n",
            "\n",
            "( 100000 ) EN:  And the 70 year-old ones , actually they &apos;re better at scouting out the good nesting places , and they also have more progeny every year .\n",
            "\n",
            "( 100000 ) VI:  Và những con 70 tuổi , thực sự giỏi hơn trong việc tìm kiếm một nơi để dựng tổ , và chúng cũng có nhiều con hơn hàng năm\n",
            "\n",
            "( 110000 ) EN:  The next time you dine on sushi -- or sashimi , or swordfish steak , or shrimp cocktail , whatever wildlife you happen to enjoy from the ocean -- think of the real cost .\n",
            "\n",
            "( 110000 ) VI:  Khi bạn thưởng thức sushi , hay sashimi , hay thịt cá kiếm nướng , hay cốc-tai tôm , bất kể thứ gì hoang dã từ đại dương mà bạn thưởng thức , hãy nghĩ về cái giá thực sự phải trả .\n",
            "\n",
            "( 120000 ) EN:  When I laid out my plan , I realized that I faced three main challenges : first , creating a sensor ; second , designing a circuit ; and third , coding a smartphone app .\n",
            "\n",
            "( 120000 ) VI:  Khi lập kế hoạch , tôi nhận ra mình đối mặt với 3 thách thức : thứ nhất , tạo ra một cảm biến ; thứ hai , thiết kế bảng mạch ; thứ ba , lập trình ứng dụng .\n",
            "\n",
            "( 130000 ) EN:  Why would you do something that dangerous ?\n",
            "\n",
            "( 130000 ) VI:  Tại sao bạn lại sẵn sàng làm một việc nguy hiểm như thế ?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sua1PaHGabby",
        "colab_type": "code",
        "outputId": "2b991c6e-32fb-4d0d-d742-181ad1a62060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-25 14:16:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-25 14:16:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-25 14:16:10--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  2.00MB/s    in 6m 28s  \n",
            "\n",
            "2020-05-25 14:22:38 (2.12 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLpl2TsqnSH9",
        "colab_type": "code",
        "outputId": "b64a252d-9cfa-4ca6-e758-1b1302002ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "!apt install gzip\n",
        "import gzip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "gzip is already the newest version (1.6-5ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZBi6z6uMsVD",
        "colab_type": "code",
        "outputId": "e4800272-85c0-45c7-ea1a-fe828ccbe07d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6B.200.dat\t  6B.50_words.pkl    glove.6B.zip    train.vi\t   tst2013.vi.1\n",
            "6B.200_idx.pkl\t  glove.6B.100d.txt  glove.6B.zip.1  train.vi.1\n",
            "6B.200_words.pkl  glove.6B.200d.txt  sample_data     tst2013.en\n",
            "6B.50.dat\t  glove.6B.300d.txt  train.en\t     tst2013.en.1\n",
            "6B.50_idx.pkl\t  glove.6B.50d.txt   train.en.1      tst2013.vi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-rFbKdODWLS",
        "colab_type": "code",
        "outputId": "21e473d0-0d47-4604-8489-8063b94a406e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install bcolz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bcolz in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.18.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc5OWKT9DkUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bcolz\n",
        "import pickle\n",
        "import copy\n",
        "import operator\n",
        "from pandas import DataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOcxhfJUDoCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = bcolz.carray(np.zeros(1), rootdir='/content/6B.200.dat', mode='w')\n",
        "with open('/content/glove.6B.200d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "     \n",
        "vectors = bcolz.carray(vectors[1:].reshape((400000, 200)), rootdir='/content/6B.200.dat', mode='w')\n",
        "vectors.flush()\n",
        "pickle.dump(words, open('/content/6B.200_words.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open('/content/6B.200_idx.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm13VvTBDriu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = bcolz.open('/content/6B.200.dat')[:]\n",
        "words = pickle.load(open('/content/6B.200_words.pkl', 'rb'))\n",
        "word2idx = pickle.load(open('/content/6B.200_idx.pkl', 'rb'))\n",
        "\n",
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkvfUMMADx9t",
        "colab_type": "code",
        "outputId": "07733a63-12a9-40bc-a231-6ab12d00db71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "glove_dframe = DataFrame(vectors, columns=range(1,201), index=words)\n",
        "print(glove_dframe.shape)\n",
        "glove_dframe[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400000, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.071549</td>\n",
              "      <td>0.093459</td>\n",
              "      <td>0.023738</td>\n",
              "      <td>-0.090339</td>\n",
              "      <td>0.056123</td>\n",
              "      <td>0.32547</td>\n",
              "      <td>-0.397960</td>\n",
              "      <td>-0.092139</td>\n",
              "      <td>0.061181</td>\n",
              "      <td>-0.189500</td>\n",
              "      <td>0.130610</td>\n",
              "      <td>0.14349</td>\n",
              "      <td>0.011479</td>\n",
              "      <td>0.381580</td>\n",
              "      <td>0.540300</td>\n",
              "      <td>-0.140880</td>\n",
              "      <td>0.243150</td>\n",
              "      <td>0.230360</td>\n",
              "      <td>-0.553390</td>\n",
              "      <td>0.048154</td>\n",
              "      <td>0.456620</td>\n",
              "      <td>3.2338</td>\n",
              "      <td>0.020199</td>\n",
              "      <td>0.049019</td>\n",
              "      <td>-0.014132</td>\n",
              "      <td>0.076017</td>\n",
              "      <td>-0.115270</td>\n",
              "      <td>0.200600</td>\n",
              "      <td>-0.077657</td>\n",
              "      <td>0.243280</td>\n",
              "      <td>0.163680</td>\n",
              "      <td>-0.341180</td>\n",
              "      <td>-0.066070</td>\n",
              "      <td>0.101520</td>\n",
              "      <td>0.038232</td>\n",
              "      <td>-0.176680</td>\n",
              "      <td>-0.88153</td>\n",
              "      <td>-0.33895</td>\n",
              "      <td>-0.035481</td>\n",
              "      <td>-0.550950</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042910</td>\n",
              "      <td>-0.067897</td>\n",
              "      <td>-0.293320</td>\n",
              "      <td>0.109780</td>\n",
              "      <td>-0.045365</td>\n",
              "      <td>0.232220</td>\n",
              "      <td>-0.311340</td>\n",
              "      <td>-0.289830</td>\n",
              "      <td>-0.666870</td>\n",
              "      <td>0.53097</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>0.366700</td>\n",
              "      <td>0.26185</td>\n",
              "      <td>-0.651870</td>\n",
              "      <td>0.102660</td>\n",
              "      <td>0.113630</td>\n",
              "      <td>-0.129530</td>\n",
              "      <td>-0.682460</td>\n",
              "      <td>-0.187510</td>\n",
              "      <td>0.147600</td>\n",
              "      <td>1.07650</td>\n",
              "      <td>-0.229080</td>\n",
              "      <td>-0.009343</td>\n",
              "      <td>-0.206510</td>\n",
              "      <td>-0.352250</td>\n",
              "      <td>-0.267200</td>\n",
              "      <td>-0.003431</td>\n",
              "      <td>0.25906</td>\n",
              "      <td>0.217590</td>\n",
              "      <td>0.661580</td>\n",
              "      <td>0.121800</td>\n",
              "      <td>0.199570</td>\n",
              "      <td>-0.20303</td>\n",
              "      <td>0.344740</td>\n",
              "      <td>-0.243280</td>\n",
              "      <td>0.131390</td>\n",
              "      <td>-0.008877</td>\n",
              "      <td>0.336170</td>\n",
              "      <td>0.030591</td>\n",
              "      <td>0.255770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>,</th>\n",
              "      <td>0.176510</td>\n",
              "      <td>0.292080</td>\n",
              "      <td>-0.002077</td>\n",
              "      <td>-0.375230</td>\n",
              "      <td>0.004914</td>\n",
              "      <td>0.23979</td>\n",
              "      <td>-0.288930</td>\n",
              "      <td>-0.014643</td>\n",
              "      <td>-0.109930</td>\n",
              "      <td>0.155920</td>\n",
              "      <td>0.206270</td>\n",
              "      <td>0.47675</td>\n",
              "      <td>0.099907</td>\n",
              "      <td>-0.140580</td>\n",
              "      <td>0.211140</td>\n",
              "      <td>0.121260</td>\n",
              "      <td>-0.318310</td>\n",
              "      <td>-0.089433</td>\n",
              "      <td>-0.090553</td>\n",
              "      <td>-0.319620</td>\n",
              "      <td>0.213190</td>\n",
              "      <td>2.4844</td>\n",
              "      <td>-0.077521</td>\n",
              "      <td>-0.084279</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>0.260840</td>\n",
              "      <td>-0.404110</td>\n",
              "      <td>-0.191270</td>\n",
              "      <td>0.247150</td>\n",
              "      <td>0.223940</td>\n",
              "      <td>-0.063437</td>\n",
              "      <td>0.203790</td>\n",
              "      <td>-0.184630</td>\n",
              "      <td>-0.088413</td>\n",
              "      <td>0.024169</td>\n",
              "      <td>-0.287690</td>\n",
              "      <td>-0.61246</td>\n",
              "      <td>-0.12683</td>\n",
              "      <td>-0.088273</td>\n",
              "      <td>0.183310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026823</td>\n",
              "      <td>-0.045444</td>\n",
              "      <td>-0.226420</td>\n",
              "      <td>-0.199770</td>\n",
              "      <td>-0.121380</td>\n",
              "      <td>0.169410</td>\n",
              "      <td>0.061998</td>\n",
              "      <td>0.426310</td>\n",
              "      <td>-0.088383</td>\n",
              "      <td>0.45756</td>\n",
              "      <td>0.077774</td>\n",
              "      <td>0.061342</td>\n",
              "      <td>0.45710</td>\n",
              "      <td>-0.177870</td>\n",
              "      <td>-0.145970</td>\n",
              "      <td>0.326540</td>\n",
              "      <td>0.002443</td>\n",
              "      <td>-0.118860</td>\n",
              "      <td>0.100810</td>\n",
              "      <td>-0.020011</td>\n",
              "      <td>1.03660</td>\n",
              "      <td>-0.398140</td>\n",
              "      <td>-0.681800</td>\n",
              "      <td>0.236850</td>\n",
              "      <td>-0.203960</td>\n",
              "      <td>-0.176680</td>\n",
              "      <td>-0.313850</td>\n",
              "      <td>0.14834</td>\n",
              "      <td>-0.052187</td>\n",
              "      <td>0.061300</td>\n",
              "      <td>-0.325820</td>\n",
              "      <td>0.191530</td>\n",
              "      <td>-0.15469</td>\n",
              "      <td>-0.146790</td>\n",
              "      <td>0.046971</td>\n",
              "      <td>0.032325</td>\n",
              "      <td>-0.220060</td>\n",
              "      <td>-0.207740</td>\n",
              "      <td>-0.231890</td>\n",
              "      <td>-0.108140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.122890</td>\n",
              "      <td>0.580370</td>\n",
              "      <td>-0.069635</td>\n",
              "      <td>-0.502880</td>\n",
              "      <td>0.105030</td>\n",
              "      <td>0.39945</td>\n",
              "      <td>-0.386350</td>\n",
              "      <td>-0.084279</td>\n",
              "      <td>0.122190</td>\n",
              "      <td>0.080312</td>\n",
              "      <td>0.323370</td>\n",
              "      <td>0.47579</td>\n",
              "      <td>-0.038375</td>\n",
              "      <td>-0.007090</td>\n",
              "      <td>0.415240</td>\n",
              "      <td>0.321210</td>\n",
              "      <td>-0.211850</td>\n",
              "      <td>0.361440</td>\n",
              "      <td>-0.055623</td>\n",
              "      <td>-0.030512</td>\n",
              "      <td>0.428540</td>\n",
              "      <td>2.8547</td>\n",
              "      <td>-0.146230</td>\n",
              "      <td>-0.175570</td>\n",
              "      <td>0.311970</td>\n",
              "      <td>-0.131180</td>\n",
              "      <td>0.033298</td>\n",
              "      <td>0.130930</td>\n",
              "      <td>0.089889</td>\n",
              "      <td>-0.124170</td>\n",
              "      <td>0.002340</td>\n",
              "      <td>-0.068954</td>\n",
              "      <td>-0.107540</td>\n",
              "      <td>-0.115510</td>\n",
              "      <td>-0.310520</td>\n",
              "      <td>-0.120970</td>\n",
              "      <td>-0.46691</td>\n",
              "      <td>-0.08360</td>\n",
              "      <td>-0.037664</td>\n",
              "      <td>-0.071779</td>\n",
              "      <td>...</td>\n",
              "      <td>0.075441</td>\n",
              "      <td>0.082116</td>\n",
              "      <td>-0.460080</td>\n",
              "      <td>0.012393</td>\n",
              "      <td>-0.025310</td>\n",
              "      <td>0.141770</td>\n",
              "      <td>-0.092192</td>\n",
              "      <td>0.345050</td>\n",
              "      <td>-0.521360</td>\n",
              "      <td>0.57304</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>0.033196</td>\n",
              "      <td>0.29672</td>\n",
              "      <td>-0.278990</td>\n",
              "      <td>0.199790</td>\n",
              "      <td>0.256660</td>\n",
              "      <td>0.082079</td>\n",
              "      <td>-0.078436</td>\n",
              "      <td>0.093719</td>\n",
              "      <td>0.242020</td>\n",
              "      <td>1.34950</td>\n",
              "      <td>-0.304340</td>\n",
              "      <td>-0.309360</td>\n",
              "      <td>0.420470</td>\n",
              "      <td>-0.079068</td>\n",
              "      <td>-0.148190</td>\n",
              "      <td>-0.089404</td>\n",
              "      <td>0.06680</td>\n",
              "      <td>0.224050</td>\n",
              "      <td>0.272260</td>\n",
              "      <td>-0.035236</td>\n",
              "      <td>0.176880</td>\n",
              "      <td>-0.05360</td>\n",
              "      <td>0.007003</td>\n",
              "      <td>-0.033006</td>\n",
              "      <td>-0.080021</td>\n",
              "      <td>-0.244510</td>\n",
              "      <td>-0.039174</td>\n",
              "      <td>-0.162360</td>\n",
              "      <td>-0.096652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.052924</td>\n",
              "      <td>0.254270</td>\n",
              "      <td>0.313530</td>\n",
              "      <td>-0.356130</td>\n",
              "      <td>0.029629</td>\n",
              "      <td>0.51034</td>\n",
              "      <td>-0.107160</td>\n",
              "      <td>0.151950</td>\n",
              "      <td>0.057698</td>\n",
              "      <td>0.061490</td>\n",
              "      <td>0.061160</td>\n",
              "      <td>0.39911</td>\n",
              "      <td>-0.000290</td>\n",
              "      <td>0.319780</td>\n",
              "      <td>0.432570</td>\n",
              "      <td>-0.147080</td>\n",
              "      <td>0.054842</td>\n",
              "      <td>0.270790</td>\n",
              "      <td>-0.140510</td>\n",
              "      <td>-0.301010</td>\n",
              "      <td>0.163130</td>\n",
              "      <td>3.0013</td>\n",
              "      <td>0.222310</td>\n",
              "      <td>-0.142790</td>\n",
              "      <td>0.083705</td>\n",
              "      <td>0.089866</td>\n",
              "      <td>-0.527060</td>\n",
              "      <td>-0.089661</td>\n",
              "      <td>0.273110</td>\n",
              "      <td>0.314130</td>\n",
              "      <td>-0.040810</td>\n",
              "      <td>0.060557</td>\n",
              "      <td>-0.042656</td>\n",
              "      <td>0.241780</td>\n",
              "      <td>-0.291870</td>\n",
              "      <td>0.225750</td>\n",
              "      <td>-0.62980</td>\n",
              "      <td>-0.14641</td>\n",
              "      <td>-0.224290</td>\n",
              "      <td>-0.056621</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405320</td>\n",
              "      <td>-0.027960</td>\n",
              "      <td>-0.133980</td>\n",
              "      <td>-0.110860</td>\n",
              "      <td>0.059506</td>\n",
              "      <td>0.240520</td>\n",
              "      <td>-0.597390</td>\n",
              "      <td>-0.002407</td>\n",
              "      <td>-0.185930</td>\n",
              "      <td>1.04200</td>\n",
              "      <td>-0.129690</td>\n",
              "      <td>0.208130</td>\n",
              "      <td>0.33305</td>\n",
              "      <td>-0.127800</td>\n",
              "      <td>0.085662</td>\n",
              "      <td>-0.076422</td>\n",
              "      <td>0.314070</td>\n",
              "      <td>-0.237840</td>\n",
              "      <td>-0.054838</td>\n",
              "      <td>0.011369</td>\n",
              "      <td>0.84500</td>\n",
              "      <td>-0.341650</td>\n",
              "      <td>0.093983</td>\n",
              "      <td>0.082445</td>\n",
              "      <td>-0.277770</td>\n",
              "      <td>-0.442260</td>\n",
              "      <td>-0.063078</td>\n",
              "      <td>0.37274</td>\n",
              "      <td>0.054468</td>\n",
              "      <td>0.241970</td>\n",
              "      <td>-0.040886</td>\n",
              "      <td>0.389400</td>\n",
              "      <td>-0.10509</td>\n",
              "      <td>0.233720</td>\n",
              "      <td>0.096027</td>\n",
              "      <td>-0.303240</td>\n",
              "      <td>0.244880</td>\n",
              "      <td>-0.086254</td>\n",
              "      <td>-0.419170</td>\n",
              "      <td>0.464960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.573460</td>\n",
              "      <td>0.541700</td>\n",
              "      <td>-0.234770</td>\n",
              "      <td>-0.362400</td>\n",
              "      <td>0.403700</td>\n",
              "      <td>0.11386</td>\n",
              "      <td>-0.449330</td>\n",
              "      <td>-0.309910</td>\n",
              "      <td>-0.005341</td>\n",
              "      <td>0.584260</td>\n",
              "      <td>-0.025956</td>\n",
              "      <td>0.49393</td>\n",
              "      <td>-0.037209</td>\n",
              "      <td>-0.284280</td>\n",
              "      <td>0.097696</td>\n",
              "      <td>-0.489070</td>\n",
              "      <td>0.026027</td>\n",
              "      <td>0.376490</td>\n",
              "      <td>0.057788</td>\n",
              "      <td>-0.468070</td>\n",
              "      <td>0.081288</td>\n",
              "      <td>3.2825</td>\n",
              "      <td>-0.636900</td>\n",
              "      <td>0.379560</td>\n",
              "      <td>0.003817</td>\n",
              "      <td>0.093607</td>\n",
              "      <td>-0.128550</td>\n",
              "      <td>0.173800</td>\n",
              "      <td>0.105220</td>\n",
              "      <td>0.286480</td>\n",
              "      <td>0.210890</td>\n",
              "      <td>-0.470760</td>\n",
              "      <td>0.027733</td>\n",
              "      <td>-0.198030</td>\n",
              "      <td>0.076328</td>\n",
              "      <td>-0.846290</td>\n",
              "      <td>-0.79708</td>\n",
              "      <td>-0.38743</td>\n",
              "      <td>-0.030422</td>\n",
              "      <td>-0.268490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.124130</td>\n",
              "      <td>-0.344310</td>\n",
              "      <td>-0.232960</td>\n",
              "      <td>-0.211870</td>\n",
              "      <td>0.085387</td>\n",
              "      <td>0.070063</td>\n",
              "      <td>-0.198030</td>\n",
              "      <td>-0.026023</td>\n",
              "      <td>-0.390370</td>\n",
              "      <td>0.80002</td>\n",
              "      <td>0.405770</td>\n",
              "      <td>-0.079863</td>\n",
              "      <td>0.35263</td>\n",
              "      <td>-0.340430</td>\n",
              "      <td>0.396760</td>\n",
              "      <td>0.228620</td>\n",
              "      <td>-0.350280</td>\n",
              "      <td>-0.473440</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.116570</td>\n",
              "      <td>1.05520</td>\n",
              "      <td>-0.415700</td>\n",
              "      <td>-0.080552</td>\n",
              "      <td>-0.056571</td>\n",
              "      <td>-0.166220</td>\n",
              "      <td>0.192740</td>\n",
              "      <td>-0.095175</td>\n",
              "      <td>-0.20781</td>\n",
              "      <td>0.156200</td>\n",
              "      <td>0.050231</td>\n",
              "      <td>-0.279150</td>\n",
              "      <td>0.437420</td>\n",
              "      <td>-0.31237</td>\n",
              "      <td>0.131940</td>\n",
              "      <td>-0.332780</td>\n",
              "      <td>0.188770</td>\n",
              "      <td>-0.234220</td>\n",
              "      <td>0.544180</td>\n",
              "      <td>-0.230690</td>\n",
              "      <td>0.349470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>0.203270</td>\n",
              "      <td>0.473480</td>\n",
              "      <td>0.050877</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.060547</td>\n",
              "      <td>0.33066</td>\n",
              "      <td>0.048486</td>\n",
              "      <td>0.021504</td>\n",
              "      <td>-0.536310</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.199830</td>\n",
              "      <td>0.51408</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.094641</td>\n",
              "      <td>0.068724</td>\n",
              "      <td>0.274240</td>\n",
              "      <td>-0.204930</td>\n",
              "      <td>0.232680</td>\n",
              "      <td>0.324900</td>\n",
              "      <td>-0.194440</td>\n",
              "      <td>0.646930</td>\n",
              "      <td>2.8342</td>\n",
              "      <td>0.140040</td>\n",
              "      <td>-0.268680</td>\n",
              "      <td>0.273250</td>\n",
              "      <td>0.015312</td>\n",
              "      <td>-0.279750</td>\n",
              "      <td>-0.264230</td>\n",
              "      <td>0.141830</td>\n",
              "      <td>-0.026064</td>\n",
              "      <td>0.113490</td>\n",
              "      <td>0.250390</td>\n",
              "      <td>-0.249720</td>\n",
              "      <td>-0.168820</td>\n",
              "      <td>-0.310390</td>\n",
              "      <td>-0.444580</td>\n",
              "      <td>-0.34789</td>\n",
              "      <td>-0.20181</td>\n",
              "      <td>-0.013405</td>\n",
              "      <td>0.236350</td>\n",
              "      <td>...</td>\n",
              "      <td>0.573390</td>\n",
              "      <td>0.180440</td>\n",
              "      <td>-0.117810</td>\n",
              "      <td>0.351620</td>\n",
              "      <td>0.162200</td>\n",
              "      <td>0.554830</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.232100</td>\n",
              "      <td>-0.202050</td>\n",
              "      <td>0.60227</td>\n",
              "      <td>-0.153790</td>\n",
              "      <td>0.219070</td>\n",
              "      <td>0.28405</td>\n",
              "      <td>0.011906</td>\n",
              "      <td>0.106220</td>\n",
              "      <td>0.506700</td>\n",
              "      <td>-0.432010</td>\n",
              "      <td>-0.408870</td>\n",
              "      <td>-0.178190</td>\n",
              "      <td>0.220420</td>\n",
              "      <td>1.07750</td>\n",
              "      <td>-0.393810</td>\n",
              "      <td>-0.358280</td>\n",
              "      <td>0.363020</td>\n",
              "      <td>0.148720</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.030339</td>\n",
              "      <td>-0.11273</td>\n",
              "      <td>0.023382</td>\n",
              "      <td>0.159040</td>\n",
              "      <td>-0.143890</td>\n",
              "      <td>-0.117540</td>\n",
              "      <td>-0.63655</td>\n",
              "      <td>-0.121970</td>\n",
              "      <td>0.043809</td>\n",
              "      <td>0.147160</td>\n",
              "      <td>0.073750</td>\n",
              "      <td>-0.213580</td>\n",
              "      <td>-0.622490</td>\n",
              "      <td>0.143860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>-0.102720</td>\n",
              "      <td>0.304100</td>\n",
              "      <td>-0.135770</td>\n",
              "      <td>-0.279790</td>\n",
              "      <td>-0.409260</td>\n",
              "      <td>-0.26553</td>\n",
              "      <td>0.104920</td>\n",
              "      <td>-0.044101</td>\n",
              "      <td>0.062731</td>\n",
              "      <td>-0.041600</td>\n",
              "      <td>0.355880</td>\n",
              "      <td>0.40757</td>\n",
              "      <td>-0.142950</td>\n",
              "      <td>-0.036534</td>\n",
              "      <td>0.425120</td>\n",
              "      <td>0.014823</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.291500</td>\n",
              "      <td>-0.204160</td>\n",
              "      <td>-0.100390</td>\n",
              "      <td>0.307670</td>\n",
              "      <td>3.1815</td>\n",
              "      <td>0.045614</td>\n",
              "      <td>0.094457</td>\n",
              "      <td>0.255450</td>\n",
              "      <td>0.275280</td>\n",
              "      <td>-0.299390</td>\n",
              "      <td>0.045123</td>\n",
              "      <td>0.446810</td>\n",
              "      <td>0.015012</td>\n",
              "      <td>-0.107850</td>\n",
              "      <td>-0.389880</td>\n",
              "      <td>-0.205620</td>\n",
              "      <td>0.263060</td>\n",
              "      <td>-0.018163</td>\n",
              "      <td>-0.142440</td>\n",
              "      <td>-0.56610</td>\n",
              "      <td>-0.12168</td>\n",
              "      <td>0.287490</td>\n",
              "      <td>-0.305810</td>\n",
              "      <td>...</td>\n",
              "      <td>0.208250</td>\n",
              "      <td>-0.171630</td>\n",
              "      <td>-0.320250</td>\n",
              "      <td>-0.275370</td>\n",
              "      <td>-0.307740</td>\n",
              "      <td>0.181840</td>\n",
              "      <td>-0.026260</td>\n",
              "      <td>-0.062733</td>\n",
              "      <td>-0.436660</td>\n",
              "      <td>0.57954</td>\n",
              "      <td>-0.323480</td>\n",
              "      <td>-0.134570</td>\n",
              "      <td>0.38565</td>\n",
              "      <td>-0.301980</td>\n",
              "      <td>0.261970</td>\n",
              "      <td>-0.100340</td>\n",
              "      <td>-0.146000</td>\n",
              "      <td>-0.341700</td>\n",
              "      <td>0.166710</td>\n",
              "      <td>-0.235990</td>\n",
              "      <td>1.24670</td>\n",
              "      <td>-0.005984</td>\n",
              "      <td>-0.569670</td>\n",
              "      <td>0.526400</td>\n",
              "      <td>-0.210240</td>\n",
              "      <td>-0.298610</td>\n",
              "      <td>-0.293000</td>\n",
              "      <td>0.15889</td>\n",
              "      <td>0.172540</td>\n",
              "      <td>-0.002398</td>\n",
              "      <td>0.071749</td>\n",
              "      <td>0.053166</td>\n",
              "      <td>-0.23429</td>\n",
              "      <td>-0.084927</td>\n",
              "      <td>0.155390</td>\n",
              "      <td>0.418200</td>\n",
              "      <td>-0.152160</td>\n",
              "      <td>0.369510</td>\n",
              "      <td>0.190390</td>\n",
              "      <td>-0.122660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.241690</td>\n",
              "      <td>-0.345340</td>\n",
              "      <td>-0.223070</td>\n",
              "      <td>-1.290700</td>\n",
              "      <td>0.252850</td>\n",
              "      <td>-0.55128</td>\n",
              "      <td>-0.080336</td>\n",
              "      <td>-0.008177</td>\n",
              "      <td>0.311360</td>\n",
              "      <td>-0.451010</td>\n",
              "      <td>0.246610</td>\n",
              "      <td>0.36441</td>\n",
              "      <td>0.943360</td>\n",
              "      <td>-0.035420</td>\n",
              "      <td>0.780480</td>\n",
              "      <td>-0.397650</td>\n",
              "      <td>0.311250</td>\n",
              "      <td>-0.177430</td>\n",
              "      <td>-0.419890</td>\n",
              "      <td>-0.378150</td>\n",
              "      <td>0.672300</td>\n",
              "      <td>3.1716</td>\n",
              "      <td>0.032496</td>\n",
              "      <td>-0.031640</td>\n",
              "      <td>0.580680</td>\n",
              "      <td>-0.444580</td>\n",
              "      <td>-0.055612</td>\n",
              "      <td>0.180520</td>\n",
              "      <td>0.285720</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.214370</td>\n",
              "      <td>0.049731</td>\n",
              "      <td>0.187200</td>\n",
              "      <td>0.119140</td>\n",
              "      <td>0.027408</td>\n",
              "      <td>-0.806080</td>\n",
              "      <td>-0.30835</td>\n",
              "      <td>-0.89737</td>\n",
              "      <td>-0.197720</td>\n",
              "      <td>0.026741</td>\n",
              "      <td>...</td>\n",
              "      <td>0.092912</td>\n",
              "      <td>-0.060809</td>\n",
              "      <td>0.029073</td>\n",
              "      <td>-0.387350</td>\n",
              "      <td>-0.070853</td>\n",
              "      <td>-0.659750</td>\n",
              "      <td>-0.381570</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>-0.735600</td>\n",
              "      <td>0.41521</td>\n",
              "      <td>0.213280</td>\n",
              "      <td>-0.337790</td>\n",
              "      <td>0.66902</td>\n",
              "      <td>0.424860</td>\n",
              "      <td>-0.121480</td>\n",
              "      <td>-0.010626</td>\n",
              "      <td>0.127450</td>\n",
              "      <td>-0.135610</td>\n",
              "      <td>0.234230</td>\n",
              "      <td>0.351100</td>\n",
              "      <td>1.28410</td>\n",
              "      <td>0.129820</td>\n",
              "      <td>0.213570</td>\n",
              "      <td>0.328570</td>\n",
              "      <td>0.165670</td>\n",
              "      <td>-0.214580</td>\n",
              "      <td>-0.442750</td>\n",
              "      <td>0.32850</td>\n",
              "      <td>0.180010</td>\n",
              "      <td>0.064865</td>\n",
              "      <td>-0.358800</td>\n",
              "      <td>-0.014226</td>\n",
              "      <td>0.31125</td>\n",
              "      <td>-0.220490</td>\n",
              "      <td>0.032829</td>\n",
              "      <td>0.385250</td>\n",
              "      <td>-0.105120</td>\n",
              "      <td>0.278010</td>\n",
              "      <td>-0.101710</td>\n",
              "      <td>-0.071521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"</th>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.312010</td>\n",
              "      <td>-0.597680</td>\n",
              "      <td>-0.125830</td>\n",
              "      <td>-0.275240</td>\n",
              "      <td>0.29145</td>\n",
              "      <td>-0.304310</td>\n",
              "      <td>0.037122</td>\n",
              "      <td>0.944680</td>\n",
              "      <td>0.088085</td>\n",
              "      <td>-0.096273</td>\n",
              "      <td>0.40542</td>\n",
              "      <td>-0.652400</td>\n",
              "      <td>0.377160</td>\n",
              "      <td>0.530010</td>\n",
              "      <td>-0.308190</td>\n",
              "      <td>-0.274780</td>\n",
              "      <td>0.810410</td>\n",
              "      <td>0.536350</td>\n",
              "      <td>0.087590</td>\n",
              "      <td>0.352880</td>\n",
              "      <td>2.8857</td>\n",
              "      <td>-0.125050</td>\n",
              "      <td>-0.035968</td>\n",
              "      <td>0.135500</td>\n",
              "      <td>-0.299320</td>\n",
              "      <td>0.561540</td>\n",
              "      <td>-0.594290</td>\n",
              "      <td>-0.349910</td>\n",
              "      <td>0.685590</td>\n",
              "      <td>-0.115370</td>\n",
              "      <td>0.088894</td>\n",
              "      <td>-0.298290</td>\n",
              "      <td>-0.207680</td>\n",
              "      <td>0.691100</td>\n",
              "      <td>0.068548</td>\n",
              "      <td>-0.50814</td>\n",
              "      <td>-0.97722</td>\n",
              "      <td>0.035782</td>\n",
              "      <td>-0.540530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.197520</td>\n",
              "      <td>-0.254360</td>\n",
              "      <td>0.166160</td>\n",
              "      <td>-0.181350</td>\n",
              "      <td>0.163740</td>\n",
              "      <td>-0.161940</td>\n",
              "      <td>-0.320130</td>\n",
              "      <td>-0.331110</td>\n",
              "      <td>0.536620</td>\n",
              "      <td>0.53301</td>\n",
              "      <td>-0.321660</td>\n",
              "      <td>-0.643620</td>\n",
              "      <td>1.05760</td>\n",
              "      <td>-0.180190</td>\n",
              "      <td>-0.409320</td>\n",
              "      <td>-0.160840</td>\n",
              "      <td>-1.009400</td>\n",
              "      <td>0.229720</td>\n",
              "      <td>0.543100</td>\n",
              "      <td>0.058910</td>\n",
              "      <td>1.75020</td>\n",
              "      <td>-0.197530</td>\n",
              "      <td>-0.054010</td>\n",
              "      <td>0.016083</td>\n",
              "      <td>-0.555230</td>\n",
              "      <td>-0.202310</td>\n",
              "      <td>-0.326130</td>\n",
              "      <td>-0.38783</td>\n",
              "      <td>0.614280</td>\n",
              "      <td>-0.112160</td>\n",
              "      <td>0.423190</td>\n",
              "      <td>-0.447290</td>\n",
              "      <td>-0.35638</td>\n",
              "      <td>-0.326980</td>\n",
              "      <td>-0.126620</td>\n",
              "      <td>-0.288580</td>\n",
              "      <td>0.080920</td>\n",
              "      <td>0.144930</td>\n",
              "      <td>0.052563</td>\n",
              "      <td>0.750070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>'s</th>\n",
              "      <td>-0.005961</td>\n",
              "      <td>0.451480</td>\n",
              "      <td>0.004549</td>\n",
              "      <td>0.020727</td>\n",
              "      <td>0.538770</td>\n",
              "      <td>0.49453</td>\n",
              "      <td>-0.353690</td>\n",
              "      <td>-0.056286</td>\n",
              "      <td>0.057851</td>\n",
              "      <td>-0.204500</td>\n",
              "      <td>-0.306400</td>\n",
              "      <td>0.37904</td>\n",
              "      <td>0.263880</td>\n",
              "      <td>0.360360</td>\n",
              "      <td>0.687150</td>\n",
              "      <td>0.116290</td>\n",
              "      <td>-0.570050</td>\n",
              "      <td>0.084364</td>\n",
              "      <td>-0.671840</td>\n",
              "      <td>0.217060</td>\n",
              "      <td>0.600840</td>\n",
              "      <td>3.1461</td>\n",
              "      <td>0.218390</td>\n",
              "      <td>0.188300</td>\n",
              "      <td>-0.011606</td>\n",
              "      <td>0.702800</td>\n",
              "      <td>-0.054734</td>\n",
              "      <td>-0.576020</td>\n",
              "      <td>-0.505610</td>\n",
              "      <td>0.132750</td>\n",
              "      <td>-0.373490</td>\n",
              "      <td>-0.223150</td>\n",
              "      <td>0.189340</td>\n",
              "      <td>-0.388350</td>\n",
              "      <td>-0.446160</td>\n",
              "      <td>0.250780</td>\n",
              "      <td>-0.24067</td>\n",
              "      <td>-0.27303</td>\n",
              "      <td>-0.635870</td>\n",
              "      <td>0.071991</td>\n",
              "      <td>...</td>\n",
              "      <td>0.664030</td>\n",
              "      <td>-0.196290</td>\n",
              "      <td>-0.066911</td>\n",
              "      <td>0.208120</td>\n",
              "      <td>0.293660</td>\n",
              "      <td>0.330200</td>\n",
              "      <td>-0.219090</td>\n",
              "      <td>-0.279870</td>\n",
              "      <td>0.110260</td>\n",
              "      <td>0.47563</td>\n",
              "      <td>0.497420</td>\n",
              "      <td>0.071567</td>\n",
              "      <td>0.19419</td>\n",
              "      <td>-0.136880</td>\n",
              "      <td>-0.069569</td>\n",
              "      <td>0.497010</td>\n",
              "      <td>-0.843270</td>\n",
              "      <td>-0.447570</td>\n",
              "      <td>0.502530</td>\n",
              "      <td>0.605960</td>\n",
              "      <td>0.86543</td>\n",
              "      <td>0.475590</td>\n",
              "      <td>-0.009351</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>-0.599520</td>\n",
              "      <td>-0.123170</td>\n",
              "      <td>-0.303610</td>\n",
              "      <td>0.17132</td>\n",
              "      <td>0.823690</td>\n",
              "      <td>0.267900</td>\n",
              "      <td>0.334550</td>\n",
              "      <td>0.355210</td>\n",
              "      <td>-0.56247</td>\n",
              "      <td>0.372890</td>\n",
              "      <td>0.515540</td>\n",
              "      <td>-0.003640</td>\n",
              "      <td>0.076358</td>\n",
              "      <td>0.399470</td>\n",
              "      <td>0.296210</td>\n",
              "      <td>0.053627</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          1         2         3    ...       198       199       200\n",
              "the -0.071549  0.093459  0.023738  ...  0.336170  0.030591  0.255770\n",
              ",    0.176510  0.292080 -0.002077  ... -0.207740 -0.231890 -0.108140\n",
              ".    0.122890  0.580370 -0.069635  ... -0.039174 -0.162360 -0.096652\n",
              "of   0.052924  0.254270  0.313530  ... -0.086254 -0.419170  0.464960\n",
              "to   0.573460  0.541700 -0.234770  ...  0.544180 -0.230690  0.349470\n",
              "and  0.203270  0.473480  0.050877  ... -0.213580 -0.622490  0.143860\n",
              "in  -0.102720  0.304100 -0.135770  ...  0.369510  0.190390 -0.122660\n",
              "a    0.241690 -0.345340 -0.223070  ...  0.278010 -0.101710 -0.071521\n",
              "\"    0.001032  0.312010 -0.597680  ...  0.144930  0.052563  0.750070\n",
              "'s  -0.005961  0.451480  0.004549  ...  0.399470  0.296210  0.053627\n",
              "\n",
              "[10 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPbNIt2WD7Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for moving 'sos' token at index 0 and 'eos' token at index 1\n",
        " \n",
        "sos_index = word2idx['sos']\n",
        "eos_index = word2idx['eos']\n",
        "sos_swap_word = words[0]\n",
        "eos_swap_word = words[1]\n",
        " \n",
        "words[0], words[sos_index] = words[sos_index], words[0]\n",
        "words[1], words[eos_index] = words[eos_index], words[1]\n",
        "word2idx[sos_swap_word], word2idx['sos'] = word2idx['sos'], word2idx[sos_swap_word]\n",
        "word2idx[eos_swap_word], word2idx['eos'] = word2idx['eos'], word2idx[eos_swap_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8cjzqomEBoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort Word2idx\n",
        "word2idx = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaUn80e2BZXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rZDxdLL5klY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uH_VuWD5lvI",
        "colab_type": "code",
        "outputId": "2dd5323c-bfd7-4878-ee70-71fe50bda70c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhDp726q5pWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        if(name == 'eng'):\n",
        "          self.word2index = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n",
        "          self.word2index[\"unk\"] = 400000 \n",
        "          self.word2count = { word : 1 for word in words }\n",
        "          self.index2word = { i : word for word, i in word2idx.items() }\n",
        "          self.n_words = 400001\n",
        "        else:\n",
        "          self.word2index = {}\n",
        "          self.word2count = {}\n",
        "          self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "          self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1tAZFIx5sAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLMVraBNFfmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 100\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br-e3-7E6EoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(lang1, lang2, source_arr, target_arr):\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "    pairs = [];\n",
        "    for i in range(0,len(target_arr)):\n",
        "      pairs.append([normalizeString(source_arr[i]), normalizeString(target_arr[i])])\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rJpzUK52MeJ",
        "colab_type": "code",
        "outputId": "9dcae2a4-95a9-403b-fe54-35e00c64f4c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "input_lang, output_lang, pairs_train = prepareData('eng', 'vi', source_sent, target_sent)\n",
        "print(random.choice(pairs_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 133267 sentence pairs\n",
            "Trimmed to 132670 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 402460\n",
            "vi 14127\n",
            "['and here look the above question is put bluntly quot are video games art ? no . video games aren apos t art because they are quite thoroughly something else code . quot ', 'hay xem nhung cau hoi tren ay uoc hoi rat thang thung quot video game la nghe thuat u ? khong chung khong phai la nghe thuat vi chung hoan toan la thu gi o rat khac oan ma . quot ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upwI_rayXo4e",
        "colab_type": "code",
        "outputId": "dff2d952-74c3-4555-a191-c0c2b15ed4c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# create a new tensor with all embeddings for english using glove dictionary\n",
        "matrix_len = input_lang.n_words\n",
        " \n",
        "weights_matrix = np.zeros((matrix_len, 200))\n",
        "words_found = 0\n",
        "print(input_lang.n_words)\n",
        "print(len(input_lang.word2index))\n",
        "for i, word in enumerate(input_lang.word2index):\n",
        "    try: \n",
        "        weights_matrix[i-2] = glove[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i-2] = np.random.normal(scale=0.6, size=(200, ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "402460\n",
            "402459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyqU3WF700M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.input_size = input_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional = True)\n",
        "\n",
        "    def forward(self, input, hidden, cell_state):\n",
        "        # print(\"forward running\")\n",
        "        # print(self.embedding(input).size())\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        # output, (hidden, cell_state) = self.lstm(output)\n",
        "        output, (hidden, cell_state) = self.lstm(output, (hidden, cell_state))\n",
        "        # print(output.size())\n",
        "        return output, hidden, cell_state\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(2, 1, self.hidden_size, device = device)\n",
        "    def initcellstate(self):\n",
        "      return torch.zeros(2, 1, self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ9UdlQE759q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# THIS CLASS ISN'T USED IN EVALUATION\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size,bidirectional =True)\n",
        "        self.out = nn.Linear(hidden_size*2, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden,cell_state):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden,cell_state = self.lstm(output, (hidden,cell_state))\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden,cell_state\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "    def initcellstate(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PotAA8uQ7_aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 100\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size,bidirectional =True)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell_state, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        # print(attn_weights.unsqueeze(0).size(), encoder_outputs.unsqueeze(0).size())\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "        # print(embedded[0].size(),attn_applied[0].size())\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, (hidden, cell_state) = self.lstm(output, (hidden, cell_state))\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, cell_state, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(2, 1, self.hidden_size, device = device)\n",
        "    def initcellstate(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjVvbet88DNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    arr = [];\n",
        "    for word in sentence.split(' '):\n",
        "        try:\n",
        "          arr.append(lang.word2index[word])\n",
        "        except:\n",
        "          arr.append(lang.word2index[\"unk\"])\n",
        "    return arr\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZThP3EA8HRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_cell = encoder.initcellstate()\n",
        "    # print(\"setting optimizers to zero grad\")\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # print(\"INPUT SIZE: \", input_tensor.size())\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      # print(\"starting encoder for \" , ei)\n",
        "      encoder_output, encoder_hidden, encoder_cell = encoder(\n",
        "          input_tensor[ei], encoder_hidden, encoder_cell)\n",
        "      # print(encoder_output)\n",
        "      encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    for di in range(target_length):\n",
        "      # print(\"decoder for di : \", di)\n",
        "      decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(\n",
        "          decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "      if decoder_input.item() == EOS_token:\n",
        "          break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn2Gamqd8JiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdSWSxdv8NF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, pairs, print_every=1000, plot_every=100, learning_rate=0.1):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    # print(\"train Iter optimizers set\")\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    # print(\"training pairs for this iteration have been assigned\")\n",
        "    # print(\"training pairs size\")\n",
        "    # print(len(training_pairs))\n",
        "    # print(len(training_pairs[0]))\n",
        "    # print(len(training_pairs[0][0]))\n",
        "    # print(len(training_pairs[0][0]))\n",
        "    # print(training_pairs[0][0])\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        # print(iter , \" : printing iter-1 th training pair\")\n",
        "        # print(training_pair)\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        # if (iter % 100 == 0):\n",
        "        #     print(input_tensor)\n",
        "        # print(iter , \" : started training with above tensors\")\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        # print(iter,\" : current iter ended\");\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SDY5Z2G8g5L",
        "colab_type": "code",
        "outputId": "f09229d7-0aa5-4af2-8021-0a571bd24f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "hidden_size = 200\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "print(\"Encoder initialization done\")\n",
        "# attn_decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "print(\"Decoder initialization done\")\n",
        "\n",
        "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "trainIters(encoder1, attn_decoder1, 1000, pairs_train, print_every=250)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder initialization done\n",
            "Decoder initialization done\n",
            "1m 0s (- 3m 2s) (250 25%) 3.3340\n",
            "1m 59s (- 1m 59s) (500 50%) 3.0075\n",
            "3m 1s (- 1m 0s) (750 75%) 2.8548\n",
            "3m 58s (- 0m 0s) (1000 100%) 3.0584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-J0HQm5hxK8",
        "colab_type": "code",
        "outputId": "4300aec1-a222-4c7c-ee4a-71ddf4fe76a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "input_lang_test, output_lang_test, pairs_test = prepareData('eng', 'vi', test_source_sent, test_target_sent)\n",
        "print(random.choice(pairs_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 1268 sentence pairs\n",
            "Trimmed to 1265 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 400032\n",
            "vi 1104\n",
            "['as soon as we moved to new england you know that place where connor was supposed to feel so safe he bought three guns .', 'khi chung toi chuyen en new england ban biet ay noi ma conor dua vao e cam thay an toan anh ta mua ba cay sung']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9We9ZU8bre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_cell = encoder.initcellstate()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden,encoder_cell = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden,encoder_cell)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden,decoder_cell, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1,:ei + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy1dYNPb8eHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n = 5):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs_test)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DXfF12H8rNL",
        "colab_type": "code",
        "outputId": "093df221-d132-44d9-d2ba-760d29ec353c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> i had my first apartment my first little green american express card and i had a very big secret .\n",
            "= lan au tien toi co can ho the tin dung american express cua rieng minh va toi a co bi mat lon .\n",
            "<  ta la ta <EOS>\n",
            "\n",
            "> so i didn apos t give up . i continued .\n",
            "= nhung toi khong tu bo . toi tiep tuc suy nghi .\n",
            "<  ta la ta <EOS>\n",
            "\n",
            "> so what happened ?\n",
            "= vay ieu gi a xay ra ?\n",
            "<  ta la ta <EOS>\n",
            "\n",
            "> thus began my journey into modern day slavery .\n",
            "= va tu o bat au cuoc hanh trinh tien vao the gioi no le hien ai\n",
            "<  ta la ta <EOS>\n",
            "\n",
            "> and they apos re both accurate . okay but in addition voice based tests are non expert .\n",
            "= va ca hai eu chuan xac . nhung them mot ieu thu nghiem dua tren giong noi khong co tinh chuyen mon .\n",
            "<  ta la ta <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKKVX8GNBzeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(50):\n",
        "#   sent = random.choice(pairs_test)[0]\n",
        "#   output_words, attentions = evaluate(\n",
        "#       encoder1, attn_decoder1, sent)\n",
        "#   print(attentions.size(), sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc_VdBg6EzfT",
        "colab_type": "code",
        "outputId": "8e4155e3-42a3-4a7d-b88e-016a946ec211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(random.choice(pairs_test)[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = today apos s slavery is about commerce so the goods that enslaved people produce have value but the people producing them are disposable .\n",
            "output = toi toi toi toi toi toi toi toi toi toi toi toi toi co toi va . toi va . . . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-6wIjIdpVNu",
        "colab_type": "text"
      },
      "source": [
        "1. BiLSTM\n",
        "2. GloVE embedding\n",
        "3. presentation\n",
        "4. BLUE Score and how error is calculated here\n",
        "5. Figure out how attention figures are made.`\n"
      ]
    }
  ]
}