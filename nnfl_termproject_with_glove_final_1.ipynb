{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnfl_termproject_with_glove_final_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-vaish5/nnfl_term_project/blob/master/nnfl_termproject_with_glove_final_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uer1xgJvH6L6",
        "colab_type": "code",
        "outputId": "b3a22a9f-a9da-4867-a9cd-be8c352919cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n",
        "!wget https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-25 14:15:51--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13603614 (13M) [text/plain]\n",
            "Saving to: ‘train.en.1’\n",
            "\n",
            "train.en.1          100%[===================>]  12.97M  7.49MB/s    in 1.7s    \n",
            "\n",
            "2020-05-25 14:15:53 (7.49 MB/s) - ‘train.en.1’ saved [13603614/13603614]\n",
            "\n",
            "--2020-05-25 14:15:55--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18074646 (17M) [text/plain]\n",
            "Saving to: ‘train.vi.1’\n",
            "\n",
            "train.vi.1          100%[===================>]  17.24M  9.92MB/s    in 1.7s    \n",
            "\n",
            "2020-05-25 14:15:57 (9.92 MB/s) - ‘train.vi.1’ saved [18074646/18074646]\n",
            "\n",
            "--2020-05-25 14:16:00--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.en\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 132264 (129K) [text/plain]\n",
            "Saving to: ‘tst2013.en.1’\n",
            "\n",
            "tst2013.en.1        100%[===================>] 129.16K   411KB/s    in 0.3s    \n",
            "\n",
            "2020-05-25 14:16:01 (411 KB/s) - ‘tst2013.en.1’ saved [132264/132264]\n",
            "\n",
            "--2020-05-25 14:16:04--  https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/tst2013.vi\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 183855 (180K) [text/plain]\n",
            "Saving to: ‘tst2013.vi.1’\n",
            "\n",
            "tst2013.vi.1        100%[===================>] 179.55K   471KB/s    in 0.4s    \n",
            "\n",
            "2020-05-25 14:16:05 (471 KB/s) - ‘tst2013.vi.1’ saved [183855/183855]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "advDQCZZIi60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlaQsvAUO9Jm",
        "colab_type": "code",
        "outputId": "572e6a33-ba76-4a85-bc55-572aa317a346",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "source_sent = []\n",
        "target_sent = []\n",
        "\n",
        "test_source_sent = []\n",
        "test_target_sent = []\n",
        "\n",
        "\n",
        "with open('train.en', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        # discarding first 20 translations as there was some\n",
        "        # english to english translations found in the first few. which are wrong\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        source_sent.append(line)\n",
        "        \n",
        "            \n",
        "with open('train.vi', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        if l_i<50:\n",
        "            continue\n",
        "        target_sent.append(line)\n",
        "\n",
        "\n",
        "with open('tst2013.en', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        test_source_sent.append(line)\n",
        "                    \n",
        "with open('tst2013.vi', encoding='utf-8') as f:\n",
        "    for l_i, line in enumerate(f):\n",
        "        test_target_sent.append(line)\n",
        "            \n",
        "assert len(source_sent) == len(target_sent),'Source: %d, Target: %d'%(len(source_sent),len(target_sent))\n",
        "assert len(test_source_sent) == len(test_target_sent), 'Source: %d, Target: %d'%(len(test_source_sent), len(test_target_sent))\n",
        "print('Sample translations (%d)'%len(source_sent))\n",
        "for i in range(0,len(source_sent),10000):\n",
        "    print('(',i,') EN: ', source_sent[i])\n",
        "    print('(',i,') VI: ', target_sent[i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample translations (133267)\n",
            "( 0 ) EN:  In each one of those assessments that we write , we always tag on a summary , and the summary is written for a non-scientific audience .\n",
            "\n",
            "( 0 ) VI:  Trong mỗi bản đánh giá chúng tôi viết , chúng tôi luôn đính kèm một bản tóm lược , được viết cho những độc giả không chuyên về khoa học .\n",
            "\n",
            "( 10000 ) EN:  This is an area in the prefrontal cortex , a region where we can use cognition to try to overcome aversive emotional states .\n",
            "\n",
            "( 10000 ) VI:  Đây là một khu vực trong vỏ não trước trán , vùng mà chúng sử dụng tri thức cho việc thử vượt qua trạng thái cảm xúc ác cảm .\n",
            "\n",
            "( 20000 ) EN:  And there are flowers that are self-infertile . That means they can &apos;t -- the pollen in their bloom can &apos;t fertilize themselves .\n",
            "\n",
            "( 20000 ) VI:  có những loài hoa không thể tự thụ phấn . Nghĩa là chúng không thể -- phấn hoa của nó không thể tụ thụ phấn được\n",
            "\n",
            "( 30000 ) EN:  And a lot of this comes together in a philosophy of change that I find really is powerful .\n",
            "\n",
            "( 30000 ) VI:  Và nhiều như vậy hợp lại thành một triết lý của sự thay đổi mà tôi thấy là thực sự rất mạnh .\n",
            "\n",
            "( 40000 ) EN:  Dean Ornish : At first for a long time , I wrote messages in notebooks .\n",
            "\n",
            "( 40000 ) VI:  Dean Ornish : &quot; Trong một khoảng thời gian dài ban đầu , tôi đã viết các tin nhắn trên các cuốn ghi chú .\n",
            "\n",
            "( 50000 ) EN:  World &apos;s first bamboo bike with folding handlebars .\n",
            "\n",
            "( 50000 ) VI:  Chiếc xe đạp bằng tre đầu tiên trên thế giới với ghi đông gập .\n",
            "\n",
            "( 60000 ) EN:  We need to invest more resources into research and treatment of mental illness .\n",
            "\n",
            "( 60000 ) VI:  Chúng ta cần đầu tư nhiều nguồn lực hơn cho công cuộc nghiên cứu và chữa trị về bệnh thần kinh .\n",
            "\n",
            "( 70000 ) EN:  If we are providing knowledge and experience , we need to structure that .\n",
            "\n",
            "( 70000 ) VI:  Nếu chúng ta cung cấp kiến thức và kinh nghiệm , chúng ta cần cơ cấu nó .\n",
            "\n",
            "( 80000 ) EN:  But I say it has to be under the conditions I &apos;ve always worked : no credit , no logos , no sponsoring .\n",
            "\n",
            "( 80000 ) VI:  Nhưng tôi nói nó phải theo các điều kiện tôi luôn luôn làm không có tín dụng , không có biểu tượng , không có tài trợ .\n",
            "\n",
            "( 90000 ) EN:  What would it look like ?\n",
            "\n",
            "( 90000 ) VI:  Nó sẽ trông như thế nào ?\n",
            "\n",
            "( 100000 ) EN:  And the 70 year-old ones , actually they &apos;re better at scouting out the good nesting places , and they also have more progeny every year .\n",
            "\n",
            "( 100000 ) VI:  Và những con 70 tuổi , thực sự giỏi hơn trong việc tìm kiếm một nơi để dựng tổ , và chúng cũng có nhiều con hơn hàng năm\n",
            "\n",
            "( 110000 ) EN:  The next time you dine on sushi -- or sashimi , or swordfish steak , or shrimp cocktail , whatever wildlife you happen to enjoy from the ocean -- think of the real cost .\n",
            "\n",
            "( 110000 ) VI:  Khi bạn thưởng thức sushi , hay sashimi , hay thịt cá kiếm nướng , hay cốc-tai tôm , bất kể thứ gì hoang dã từ đại dương mà bạn thưởng thức , hãy nghĩ về cái giá thực sự phải trả .\n",
            "\n",
            "( 120000 ) EN:  When I laid out my plan , I realized that I faced three main challenges : first , creating a sensor ; second , designing a circuit ; and third , coding a smartphone app .\n",
            "\n",
            "( 120000 ) VI:  Khi lập kế hoạch , tôi nhận ra mình đối mặt với 3 thách thức : thứ nhất , tạo ra một cảm biến ; thứ hai , thiết kế bảng mạch ; thứ ba , lập trình ứng dụng .\n",
            "\n",
            "( 130000 ) EN:  Why would you do something that dangerous ?\n",
            "\n",
            "( 130000 ) VI:  Tại sao bạn lại sẵn sàng làm một việc nguy hiểm như thế ?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sua1PaHGabby",
        "colab_type": "code",
        "outputId": "2b991c6e-32fb-4d0d-d742-181ad1a62060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-25 14:16:09--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-25 14:16:09--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-25 14:16:10--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  2.00MB/s    in 6m 28s  \n",
            "\n",
            "2020-05-25 14:22:38 (2.12 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLpl2TsqnSH9",
        "colab_type": "code",
        "outputId": "b64a252d-9cfa-4ca6-e758-1b1302002ee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "!apt install gzip\n",
        "import gzip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "gzip is already the newest version (1.6-5ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZBi6z6uMsVD",
        "colab_type": "code",
        "outputId": "e4800272-85c0-45c7-ea1a-fe828ccbe07d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6B.200.dat\t  6B.50_words.pkl    glove.6B.zip    train.vi\t   tst2013.vi.1\n",
            "6B.200_idx.pkl\t  glove.6B.100d.txt  glove.6B.zip.1  train.vi.1\n",
            "6B.200_words.pkl  glove.6B.200d.txt  sample_data     tst2013.en\n",
            "6B.50.dat\t  glove.6B.300d.txt  train.en\t     tst2013.en.1\n",
            "6B.50_idx.pkl\t  glove.6B.50d.txt   train.en.1      tst2013.vi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-rFbKdODWLS",
        "colab_type": "code",
        "outputId": "21e473d0-0d47-4604-8489-8063b94a406e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!pip install bcolz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bcolz in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from bcolz) (1.18.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc5OWKT9DkUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bcolz\n",
        "import pickle\n",
        "import copy\n",
        "import operator\n",
        "from pandas import DataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOcxhfJUDoCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "idx = 0\n",
        "word2idx = {}\n",
        "vectors = bcolz.carray(np.zeros(1), rootdir='/content/6B.200.dat', mode='w')\n",
        "with open('/content/glove.6B.200d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0]\n",
        "        words.append(word)\n",
        "        word2idx[word] = idx\n",
        "        idx += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "     \n",
        "vectors = bcolz.carray(vectors[1:].reshape((400000, 200)), rootdir='/content/6B.200.dat', mode='w')\n",
        "vectors.flush()\n",
        "pickle.dump(words, open('/content/6B.200_words.pkl', 'wb'))\n",
        "pickle.dump(word2idx, open('/content/6B.200_idx.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm13VvTBDriu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = bcolz.open('/content/6B.200.dat')[:]\n",
        "words = pickle.load(open('/content/6B.200_words.pkl', 'rb'))\n",
        "word2idx = pickle.load(open('/content/6B.200_idx.pkl', 'rb'))\n",
        "\n",
        "glove = {w: vectors[word2idx[w]] for w in words}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkvfUMMADx9t",
        "colab_type": "code",
        "outputId": "07733a63-12a9-40bc-a231-6ab12d00db71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "glove_dframe = DataFrame(vectors, columns=range(1,201), index=words)\n",
        "print(glove_dframe.shape)\n",
        "glove_dframe[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(400000, 200)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>161</th>\n",
              "      <th>162</th>\n",
              "      <th>163</th>\n",
              "      <th>164</th>\n",
              "      <th>165</th>\n",
              "      <th>166</th>\n",
              "      <th>167</th>\n",
              "      <th>168</th>\n",
              "      <th>169</th>\n",
              "      <th>170</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>180</th>\n",
              "      <th>181</th>\n",
              "      <th>182</th>\n",
              "      <th>183</th>\n",
              "      <th>184</th>\n",
              "      <th>185</th>\n",
              "      <th>186</th>\n",
              "      <th>187</th>\n",
              "      <th>188</th>\n",
              "      <th>189</th>\n",
              "      <th>190</th>\n",
              "      <th>191</th>\n",
              "      <th>192</th>\n",
              "      <th>193</th>\n",
              "      <th>194</th>\n",
              "      <th>195</th>\n",
              "      <th>196</th>\n",
              "      <th>197</th>\n",
              "      <th>198</th>\n",
              "      <th>199</th>\n",
              "      <th>200</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.071549</td>\n",
              "      <td>0.093459</td>\n",
              "      <td>0.023738</td>\n",
              "      <td>-0.090339</td>\n",
              "      <td>0.056123</td>\n",
              "      <td>0.32547</td>\n",
              "      <td>-0.397960</td>\n",
              "      <td>-0.092139</td>\n",
              "      <td>0.061181</td>\n",
              "      <td>-0.189500</td>\n",
              "      <td>0.130610</td>\n",
              "      <td>0.14349</td>\n",
              "      <td>0.011479</td>\n",
              "      <td>0.381580</td>\n",
              "      <td>0.540300</td>\n",
              "      <td>-0.140880</td>\n",
              "      <td>0.243150</td>\n",
              "      <td>0.230360</td>\n",
              "      <td>-0.553390</td>\n",
              "      <td>0.048154</td>\n",
              "      <td>0.456620</td>\n",
              "      <td>3.2338</td>\n",
              "      <td>0.020199</td>\n",
              "      <td>0.049019</td>\n",
              "      <td>-0.014132</td>\n",
              "      <td>0.076017</td>\n",
              "      <td>-0.115270</td>\n",
              "      <td>0.200600</td>\n",
              "      <td>-0.077657</td>\n",
              "      <td>0.243280</td>\n",
              "      <td>0.163680</td>\n",
              "      <td>-0.341180</td>\n",
              "      <td>-0.066070</td>\n",
              "      <td>0.101520</td>\n",
              "      <td>0.038232</td>\n",
              "      <td>-0.176680</td>\n",
              "      <td>-0.88153</td>\n",
              "      <td>-0.33895</td>\n",
              "      <td>-0.035481</td>\n",
              "      <td>-0.550950</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042910</td>\n",
              "      <td>-0.067897</td>\n",
              "      <td>-0.293320</td>\n",
              "      <td>0.109780</td>\n",
              "      <td>-0.045365</td>\n",
              "      <td>0.232220</td>\n",
              "      <td>-0.311340</td>\n",
              "      <td>-0.289830</td>\n",
              "      <td>-0.666870</td>\n",
              "      <td>0.53097</td>\n",
              "      <td>0.194610</td>\n",
              "      <td>0.366700</td>\n",
              "      <td>0.26185</td>\n",
              "      <td>-0.651870</td>\n",
              "      <td>0.102660</td>\n",
              "      <td>0.113630</td>\n",
              "      <td>-0.129530</td>\n",
              "      <td>-0.682460</td>\n",
              "      <td>-0.187510</td>\n",
              "      <td>0.147600</td>\n",
              "      <td>1.07650</td>\n",
              "      <td>-0.229080</td>\n",
              "      <td>-0.009343</td>\n",
              "      <td>-0.206510</td>\n",
              "      <td>-0.352250</td>\n",
              "      <td>-0.267200</td>\n",
              "      <td>-0.003431</td>\n",
              "      <td>0.25906</td>\n",
              "      <td>0.217590</td>\n",
              "      <td>0.661580</td>\n",
              "      <td>0.121800</td>\n",
              "      <td>0.199570</td>\n",
              "      <td>-0.20303</td>\n",
              "      <td>0.344740</td>\n",
              "      <td>-0.243280</td>\n",
              "      <td>0.131390</td>\n",
              "      <td>-0.008877</td>\n",
              "      <td>0.336170</td>\n",
              "      <td>0.030591</td>\n",
              "      <td>0.255770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>,</th>\n",
              "      <td>0.176510</td>\n",
              "      <td>0.292080</td>\n",
              "      <td>-0.002077</td>\n",
              "      <td>-0.375230</td>\n",
              "      <td>0.004914</td>\n",
              "      <td>0.23979</td>\n",
              "      <td>-0.288930</td>\n",
              "      <td>-0.014643</td>\n",
              "      <td>-0.109930</td>\n",
              "      <td>0.155920</td>\n",
              "      <td>0.206270</td>\n",
              "      <td>0.47675</td>\n",
              "      <td>0.099907</td>\n",
              "      <td>-0.140580</td>\n",
              "      <td>0.211140</td>\n",
              "      <td>0.121260</td>\n",
              "      <td>-0.318310</td>\n",
              "      <td>-0.089433</td>\n",
              "      <td>-0.090553</td>\n",
              "      <td>-0.319620</td>\n",
              "      <td>0.213190</td>\n",
              "      <td>2.4844</td>\n",
              "      <td>-0.077521</td>\n",
              "      <td>-0.084279</td>\n",
              "      <td>0.201860</td>\n",
              "      <td>0.260840</td>\n",
              "      <td>-0.404110</td>\n",
              "      <td>-0.191270</td>\n",
              "      <td>0.247150</td>\n",
              "      <td>0.223940</td>\n",
              "      <td>-0.063437</td>\n",
              "      <td>0.203790</td>\n",
              "      <td>-0.184630</td>\n",
              "      <td>-0.088413</td>\n",
              "      <td>0.024169</td>\n",
              "      <td>-0.287690</td>\n",
              "      <td>-0.61246</td>\n",
              "      <td>-0.12683</td>\n",
              "      <td>-0.088273</td>\n",
              "      <td>0.183310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026823</td>\n",
              "      <td>-0.045444</td>\n",
              "      <td>-0.226420</td>\n",
              "      <td>-0.199770</td>\n",
              "      <td>-0.121380</td>\n",
              "      <td>0.169410</td>\n",
              "      <td>0.061998</td>\n",
              "      <td>0.426310</td>\n",
              "      <td>-0.088383</td>\n",
              "      <td>0.45756</td>\n",
              "      <td>0.077774</td>\n",
              "      <td>0.061342</td>\n",
              "      <td>0.45710</td>\n",
              "      <td>-0.177870</td>\n",
              "      <td>-0.145970</td>\n",
              "      <td>0.326540</td>\n",
              "      <td>0.002443</td>\n",
              "      <td>-0.118860</td>\n",
              "      <td>0.100810</td>\n",
              "      <td>-0.020011</td>\n",
              "      <td>1.03660</td>\n",
              "      <td>-0.398140</td>\n",
              "      <td>-0.681800</td>\n",
              "      <td>0.236850</td>\n",
              "      <td>-0.203960</td>\n",
              "      <td>-0.176680</td>\n",
              "      <td>-0.313850</td>\n",
              "      <td>0.14834</td>\n",
              "      <td>-0.052187</td>\n",
              "      <td>0.061300</td>\n",
              "      <td>-0.325820</td>\n",
              "      <td>0.191530</td>\n",
              "      <td>-0.15469</td>\n",
              "      <td>-0.146790</td>\n",
              "      <td>0.046971</td>\n",
              "      <td>0.032325</td>\n",
              "      <td>-0.220060</td>\n",
              "      <td>-0.207740</td>\n",
              "      <td>-0.231890</td>\n",
              "      <td>-0.108140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0.122890</td>\n",
              "      <td>0.580370</td>\n",
              "      <td>-0.069635</td>\n",
              "      <td>-0.502880</td>\n",
              "      <td>0.105030</td>\n",
              "      <td>0.39945</td>\n",
              "      <td>-0.386350</td>\n",
              "      <td>-0.084279</td>\n",
              "      <td>0.122190</td>\n",
              "      <td>0.080312</td>\n",
              "      <td>0.323370</td>\n",
              "      <td>0.47579</td>\n",
              "      <td>-0.038375</td>\n",
              "      <td>-0.007090</td>\n",
              "      <td>0.415240</td>\n",
              "      <td>0.321210</td>\n",
              "      <td>-0.211850</td>\n",
              "      <td>0.361440</td>\n",
              "      <td>-0.055623</td>\n",
              "      <td>-0.030512</td>\n",
              "      <td>0.428540</td>\n",
              "      <td>2.8547</td>\n",
              "      <td>-0.146230</td>\n",
              "      <td>-0.175570</td>\n",
              "      <td>0.311970</td>\n",
              "      <td>-0.131180</td>\n",
              "      <td>0.033298</td>\n",
              "      <td>0.130930</td>\n",
              "      <td>0.089889</td>\n",
              "      <td>-0.124170</td>\n",
              "      <td>0.002340</td>\n",
              "      <td>-0.068954</td>\n",
              "      <td>-0.107540</td>\n",
              "      <td>-0.115510</td>\n",
              "      <td>-0.310520</td>\n",
              "      <td>-0.120970</td>\n",
              "      <td>-0.46691</td>\n",
              "      <td>-0.08360</td>\n",
              "      <td>-0.037664</td>\n",
              "      <td>-0.071779</td>\n",
              "      <td>...</td>\n",
              "      <td>0.075441</td>\n",
              "      <td>0.082116</td>\n",
              "      <td>-0.460080</td>\n",
              "      <td>0.012393</td>\n",
              "      <td>-0.025310</td>\n",
              "      <td>0.141770</td>\n",
              "      <td>-0.092192</td>\n",
              "      <td>0.345050</td>\n",
              "      <td>-0.521360</td>\n",
              "      <td>0.57304</td>\n",
              "      <td>0.011973</td>\n",
              "      <td>0.033196</td>\n",
              "      <td>0.29672</td>\n",
              "      <td>-0.278990</td>\n",
              "      <td>0.199790</td>\n",
              "      <td>0.256660</td>\n",
              "      <td>0.082079</td>\n",
              "      <td>-0.078436</td>\n",
              "      <td>0.093719</td>\n",
              "      <td>0.242020</td>\n",
              "      <td>1.34950</td>\n",
              "      <td>-0.304340</td>\n",
              "      <td>-0.309360</td>\n",
              "      <td>0.420470</td>\n",
              "      <td>-0.079068</td>\n",
              "      <td>-0.148190</td>\n",
              "      <td>-0.089404</td>\n",
              "      <td>0.06680</td>\n",
              "      <td>0.224050</td>\n",
              "      <td>0.272260</td>\n",
              "      <td>-0.035236</td>\n",
              "      <td>0.176880</td>\n",
              "      <td>-0.05360</td>\n",
              "      <td>0.007003</td>\n",
              "      <td>-0.033006</td>\n",
              "      <td>-0.080021</td>\n",
              "      <td>-0.244510</td>\n",
              "      <td>-0.039174</td>\n",
              "      <td>-0.162360</td>\n",
              "      <td>-0.096652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.052924</td>\n",
              "      <td>0.254270</td>\n",
              "      <td>0.313530</td>\n",
              "      <td>-0.356130</td>\n",
              "      <td>0.029629</td>\n",
              "      <td>0.51034</td>\n",
              "      <td>-0.107160</td>\n",
              "      <td>0.151950</td>\n",
              "      <td>0.057698</td>\n",
              "      <td>0.061490</td>\n",
              "      <td>0.061160</td>\n",
              "      <td>0.39911</td>\n",
              "      <td>-0.000290</td>\n",
              "      <td>0.319780</td>\n",
              "      <td>0.432570</td>\n",
              "      <td>-0.147080</td>\n",
              "      <td>0.054842</td>\n",
              "      <td>0.270790</td>\n",
              "      <td>-0.140510</td>\n",
              "      <td>-0.301010</td>\n",
              "      <td>0.163130</td>\n",
              "      <td>3.0013</td>\n",
              "      <td>0.222310</td>\n",
              "      <td>-0.142790</td>\n",
              "      <td>0.083705</td>\n",
              "      <td>0.089866</td>\n",
              "      <td>-0.527060</td>\n",
              "      <td>-0.089661</td>\n",
              "      <td>0.273110</td>\n",
              "      <td>0.314130</td>\n",
              "      <td>-0.040810</td>\n",
              "      <td>0.060557</td>\n",
              "      <td>-0.042656</td>\n",
              "      <td>0.241780</td>\n",
              "      <td>-0.291870</td>\n",
              "      <td>0.225750</td>\n",
              "      <td>-0.62980</td>\n",
              "      <td>-0.14641</td>\n",
              "      <td>-0.224290</td>\n",
              "      <td>-0.056621</td>\n",
              "      <td>...</td>\n",
              "      <td>0.405320</td>\n",
              "      <td>-0.027960</td>\n",
              "      <td>-0.133980</td>\n",
              "      <td>-0.110860</td>\n",
              "      <td>0.059506</td>\n",
              "      <td>0.240520</td>\n",
              "      <td>-0.597390</td>\n",
              "      <td>-0.002407</td>\n",
              "      <td>-0.185930</td>\n",
              "      <td>1.04200</td>\n",
              "      <td>-0.129690</td>\n",
              "      <td>0.208130</td>\n",
              "      <td>0.33305</td>\n",
              "      <td>-0.127800</td>\n",
              "      <td>0.085662</td>\n",
              "      <td>-0.076422</td>\n",
              "      <td>0.314070</td>\n",
              "      <td>-0.237840</td>\n",
              "      <td>-0.054838</td>\n",
              "      <td>0.011369</td>\n",
              "      <td>0.84500</td>\n",
              "      <td>-0.341650</td>\n",
              "      <td>0.093983</td>\n",
              "      <td>0.082445</td>\n",
              "      <td>-0.277770</td>\n",
              "      <td>-0.442260</td>\n",
              "      <td>-0.063078</td>\n",
              "      <td>0.37274</td>\n",
              "      <td>0.054468</td>\n",
              "      <td>0.241970</td>\n",
              "      <td>-0.040886</td>\n",
              "      <td>0.389400</td>\n",
              "      <td>-0.10509</td>\n",
              "      <td>0.233720</td>\n",
              "      <td>0.096027</td>\n",
              "      <td>-0.303240</td>\n",
              "      <td>0.244880</td>\n",
              "      <td>-0.086254</td>\n",
              "      <td>-0.419170</td>\n",
              "      <td>0.464960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.573460</td>\n",
              "      <td>0.541700</td>\n",
              "      <td>-0.234770</td>\n",
              "      <td>-0.362400</td>\n",
              "      <td>0.403700</td>\n",
              "      <td>0.11386</td>\n",
              "      <td>-0.449330</td>\n",
              "      <td>-0.309910</td>\n",
              "      <td>-0.005341</td>\n",
              "      <td>0.584260</td>\n",
              "      <td>-0.025956</td>\n",
              "      <td>0.49393</td>\n",
              "      <td>-0.037209</td>\n",
              "      <td>-0.284280</td>\n",
              "      <td>0.097696</td>\n",
              "      <td>-0.489070</td>\n",
              "      <td>0.026027</td>\n",
              "      <td>0.376490</td>\n",
              "      <td>0.057788</td>\n",
              "      <td>-0.468070</td>\n",
              "      <td>0.081288</td>\n",
              "      <td>3.2825</td>\n",
              "      <td>-0.636900</td>\n",
              "      <td>0.379560</td>\n",
              "      <td>0.003817</td>\n",
              "      <td>0.093607</td>\n",
              "      <td>-0.128550</td>\n",
              "      <td>0.173800</td>\n",
              "      <td>0.105220</td>\n",
              "      <td>0.286480</td>\n",
              "      <td>0.210890</td>\n",
              "      <td>-0.470760</td>\n",
              "      <td>0.027733</td>\n",
              "      <td>-0.198030</td>\n",
              "      <td>0.076328</td>\n",
              "      <td>-0.846290</td>\n",
              "      <td>-0.79708</td>\n",
              "      <td>-0.38743</td>\n",
              "      <td>-0.030422</td>\n",
              "      <td>-0.268490</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.124130</td>\n",
              "      <td>-0.344310</td>\n",
              "      <td>-0.232960</td>\n",
              "      <td>-0.211870</td>\n",
              "      <td>0.085387</td>\n",
              "      <td>0.070063</td>\n",
              "      <td>-0.198030</td>\n",
              "      <td>-0.026023</td>\n",
              "      <td>-0.390370</td>\n",
              "      <td>0.80002</td>\n",
              "      <td>0.405770</td>\n",
              "      <td>-0.079863</td>\n",
              "      <td>0.35263</td>\n",
              "      <td>-0.340430</td>\n",
              "      <td>0.396760</td>\n",
              "      <td>0.228620</td>\n",
              "      <td>-0.350280</td>\n",
              "      <td>-0.473440</td>\n",
              "      <td>0.597420</td>\n",
              "      <td>-0.116570</td>\n",
              "      <td>1.05520</td>\n",
              "      <td>-0.415700</td>\n",
              "      <td>-0.080552</td>\n",
              "      <td>-0.056571</td>\n",
              "      <td>-0.166220</td>\n",
              "      <td>0.192740</td>\n",
              "      <td>-0.095175</td>\n",
              "      <td>-0.20781</td>\n",
              "      <td>0.156200</td>\n",
              "      <td>0.050231</td>\n",
              "      <td>-0.279150</td>\n",
              "      <td>0.437420</td>\n",
              "      <td>-0.31237</td>\n",
              "      <td>0.131940</td>\n",
              "      <td>-0.332780</td>\n",
              "      <td>0.188770</td>\n",
              "      <td>-0.234220</td>\n",
              "      <td>0.544180</td>\n",
              "      <td>-0.230690</td>\n",
              "      <td>0.349470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>0.203270</td>\n",
              "      <td>0.473480</td>\n",
              "      <td>0.050877</td>\n",
              "      <td>0.002103</td>\n",
              "      <td>0.060547</td>\n",
              "      <td>0.33066</td>\n",
              "      <td>0.048486</td>\n",
              "      <td>0.021504</td>\n",
              "      <td>-0.536310</td>\n",
              "      <td>0.213120</td>\n",
              "      <td>0.199830</td>\n",
              "      <td>0.51408</td>\n",
              "      <td>0.000704</td>\n",
              "      <td>0.094641</td>\n",
              "      <td>0.068724</td>\n",
              "      <td>0.274240</td>\n",
              "      <td>-0.204930</td>\n",
              "      <td>0.232680</td>\n",
              "      <td>0.324900</td>\n",
              "      <td>-0.194440</td>\n",
              "      <td>0.646930</td>\n",
              "      <td>2.8342</td>\n",
              "      <td>0.140040</td>\n",
              "      <td>-0.268680</td>\n",
              "      <td>0.273250</td>\n",
              "      <td>0.015312</td>\n",
              "      <td>-0.279750</td>\n",
              "      <td>-0.264230</td>\n",
              "      <td>0.141830</td>\n",
              "      <td>-0.026064</td>\n",
              "      <td>0.113490</td>\n",
              "      <td>0.250390</td>\n",
              "      <td>-0.249720</td>\n",
              "      <td>-0.168820</td>\n",
              "      <td>-0.310390</td>\n",
              "      <td>-0.444580</td>\n",
              "      <td>-0.34789</td>\n",
              "      <td>-0.20181</td>\n",
              "      <td>-0.013405</td>\n",
              "      <td>0.236350</td>\n",
              "      <td>...</td>\n",
              "      <td>0.573390</td>\n",
              "      <td>0.180440</td>\n",
              "      <td>-0.117810</td>\n",
              "      <td>0.351620</td>\n",
              "      <td>0.162200</td>\n",
              "      <td>0.554830</td>\n",
              "      <td>0.140000</td>\n",
              "      <td>0.232100</td>\n",
              "      <td>-0.202050</td>\n",
              "      <td>0.60227</td>\n",
              "      <td>-0.153790</td>\n",
              "      <td>0.219070</td>\n",
              "      <td>0.28405</td>\n",
              "      <td>0.011906</td>\n",
              "      <td>0.106220</td>\n",
              "      <td>0.506700</td>\n",
              "      <td>-0.432010</td>\n",
              "      <td>-0.408870</td>\n",
              "      <td>-0.178190</td>\n",
              "      <td>0.220420</td>\n",
              "      <td>1.07750</td>\n",
              "      <td>-0.393810</td>\n",
              "      <td>-0.358280</td>\n",
              "      <td>0.363020</td>\n",
              "      <td>0.148720</td>\n",
              "      <td>0.035555</td>\n",
              "      <td>-0.030339</td>\n",
              "      <td>-0.11273</td>\n",
              "      <td>0.023382</td>\n",
              "      <td>0.159040</td>\n",
              "      <td>-0.143890</td>\n",
              "      <td>-0.117540</td>\n",
              "      <td>-0.63655</td>\n",
              "      <td>-0.121970</td>\n",
              "      <td>0.043809</td>\n",
              "      <td>0.147160</td>\n",
              "      <td>0.073750</td>\n",
              "      <td>-0.213580</td>\n",
              "      <td>-0.622490</td>\n",
              "      <td>0.143860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>-0.102720</td>\n",
              "      <td>0.304100</td>\n",
              "      <td>-0.135770</td>\n",
              "      <td>-0.279790</td>\n",
              "      <td>-0.409260</td>\n",
              "      <td>-0.26553</td>\n",
              "      <td>0.104920</td>\n",
              "      <td>-0.044101</td>\n",
              "      <td>0.062731</td>\n",
              "      <td>-0.041600</td>\n",
              "      <td>0.355880</td>\n",
              "      <td>0.40757</td>\n",
              "      <td>-0.142950</td>\n",
              "      <td>-0.036534</td>\n",
              "      <td>0.425120</td>\n",
              "      <td>0.014823</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.291500</td>\n",
              "      <td>-0.204160</td>\n",
              "      <td>-0.100390</td>\n",
              "      <td>0.307670</td>\n",
              "      <td>3.1815</td>\n",
              "      <td>0.045614</td>\n",
              "      <td>0.094457</td>\n",
              "      <td>0.255450</td>\n",
              "      <td>0.275280</td>\n",
              "      <td>-0.299390</td>\n",
              "      <td>0.045123</td>\n",
              "      <td>0.446810</td>\n",
              "      <td>0.015012</td>\n",
              "      <td>-0.107850</td>\n",
              "      <td>-0.389880</td>\n",
              "      <td>-0.205620</td>\n",
              "      <td>0.263060</td>\n",
              "      <td>-0.018163</td>\n",
              "      <td>-0.142440</td>\n",
              "      <td>-0.56610</td>\n",
              "      <td>-0.12168</td>\n",
              "      <td>0.287490</td>\n",
              "      <td>-0.305810</td>\n",
              "      <td>...</td>\n",
              "      <td>0.208250</td>\n",
              "      <td>-0.171630</td>\n",
              "      <td>-0.320250</td>\n",
              "      <td>-0.275370</td>\n",
              "      <td>-0.307740</td>\n",
              "      <td>0.181840</td>\n",
              "      <td>-0.026260</td>\n",
              "      <td>-0.062733</td>\n",
              "      <td>-0.436660</td>\n",
              "      <td>0.57954</td>\n",
              "      <td>-0.323480</td>\n",
              "      <td>-0.134570</td>\n",
              "      <td>0.38565</td>\n",
              "      <td>-0.301980</td>\n",
              "      <td>0.261970</td>\n",
              "      <td>-0.100340</td>\n",
              "      <td>-0.146000</td>\n",
              "      <td>-0.341700</td>\n",
              "      <td>0.166710</td>\n",
              "      <td>-0.235990</td>\n",
              "      <td>1.24670</td>\n",
              "      <td>-0.005984</td>\n",
              "      <td>-0.569670</td>\n",
              "      <td>0.526400</td>\n",
              "      <td>-0.210240</td>\n",
              "      <td>-0.298610</td>\n",
              "      <td>-0.293000</td>\n",
              "      <td>0.15889</td>\n",
              "      <td>0.172540</td>\n",
              "      <td>-0.002398</td>\n",
              "      <td>0.071749</td>\n",
              "      <td>0.053166</td>\n",
              "      <td>-0.23429</td>\n",
              "      <td>-0.084927</td>\n",
              "      <td>0.155390</td>\n",
              "      <td>0.418200</td>\n",
              "      <td>-0.152160</td>\n",
              "      <td>0.369510</td>\n",
              "      <td>0.190390</td>\n",
              "      <td>-0.122660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>a</th>\n",
              "      <td>0.241690</td>\n",
              "      <td>-0.345340</td>\n",
              "      <td>-0.223070</td>\n",
              "      <td>-1.290700</td>\n",
              "      <td>0.252850</td>\n",
              "      <td>-0.55128</td>\n",
              "      <td>-0.080336</td>\n",
              "      <td>-0.008177</td>\n",
              "      <td>0.311360</td>\n",
              "      <td>-0.451010</td>\n",
              "      <td>0.246610</td>\n",
              "      <td>0.36441</td>\n",
              "      <td>0.943360</td>\n",
              "      <td>-0.035420</td>\n",
              "      <td>0.780480</td>\n",
              "      <td>-0.397650</td>\n",
              "      <td>0.311250</td>\n",
              "      <td>-0.177430</td>\n",
              "      <td>-0.419890</td>\n",
              "      <td>-0.378150</td>\n",
              "      <td>0.672300</td>\n",
              "      <td>3.1716</td>\n",
              "      <td>0.032496</td>\n",
              "      <td>-0.031640</td>\n",
              "      <td>0.580680</td>\n",
              "      <td>-0.444580</td>\n",
              "      <td>-0.055612</td>\n",
              "      <td>0.180520</td>\n",
              "      <td>0.285720</td>\n",
              "      <td>0.095870</td>\n",
              "      <td>0.214370</td>\n",
              "      <td>0.049731</td>\n",
              "      <td>0.187200</td>\n",
              "      <td>0.119140</td>\n",
              "      <td>0.027408</td>\n",
              "      <td>-0.806080</td>\n",
              "      <td>-0.30835</td>\n",
              "      <td>-0.89737</td>\n",
              "      <td>-0.197720</td>\n",
              "      <td>0.026741</td>\n",
              "      <td>...</td>\n",
              "      <td>0.092912</td>\n",
              "      <td>-0.060809</td>\n",
              "      <td>0.029073</td>\n",
              "      <td>-0.387350</td>\n",
              "      <td>-0.070853</td>\n",
              "      <td>-0.659750</td>\n",
              "      <td>-0.381570</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>-0.735600</td>\n",
              "      <td>0.41521</td>\n",
              "      <td>0.213280</td>\n",
              "      <td>-0.337790</td>\n",
              "      <td>0.66902</td>\n",
              "      <td>0.424860</td>\n",
              "      <td>-0.121480</td>\n",
              "      <td>-0.010626</td>\n",
              "      <td>0.127450</td>\n",
              "      <td>-0.135610</td>\n",
              "      <td>0.234230</td>\n",
              "      <td>0.351100</td>\n",
              "      <td>1.28410</td>\n",
              "      <td>0.129820</td>\n",
              "      <td>0.213570</td>\n",
              "      <td>0.328570</td>\n",
              "      <td>0.165670</td>\n",
              "      <td>-0.214580</td>\n",
              "      <td>-0.442750</td>\n",
              "      <td>0.32850</td>\n",
              "      <td>0.180010</td>\n",
              "      <td>0.064865</td>\n",
              "      <td>-0.358800</td>\n",
              "      <td>-0.014226</td>\n",
              "      <td>0.31125</td>\n",
              "      <td>-0.220490</td>\n",
              "      <td>0.032829</td>\n",
              "      <td>0.385250</td>\n",
              "      <td>-0.105120</td>\n",
              "      <td>0.278010</td>\n",
              "      <td>-0.101710</td>\n",
              "      <td>-0.071521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>\"</th>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.312010</td>\n",
              "      <td>-0.597680</td>\n",
              "      <td>-0.125830</td>\n",
              "      <td>-0.275240</td>\n",
              "      <td>0.29145</td>\n",
              "      <td>-0.304310</td>\n",
              "      <td>0.037122</td>\n",
              "      <td>0.944680</td>\n",
              "      <td>0.088085</td>\n",
              "      <td>-0.096273</td>\n",
              "      <td>0.40542</td>\n",
              "      <td>-0.652400</td>\n",
              "      <td>0.377160</td>\n",
              "      <td>0.530010</td>\n",
              "      <td>-0.308190</td>\n",
              "      <td>-0.274780</td>\n",
              "      <td>0.810410</td>\n",
              "      <td>0.536350</td>\n",
              "      <td>0.087590</td>\n",
              "      <td>0.352880</td>\n",
              "      <td>2.8857</td>\n",
              "      <td>-0.125050</td>\n",
              "      <td>-0.035968</td>\n",
              "      <td>0.135500</td>\n",
              "      <td>-0.299320</td>\n",
              "      <td>0.561540</td>\n",
              "      <td>-0.594290</td>\n",
              "      <td>-0.349910</td>\n",
              "      <td>0.685590</td>\n",
              "      <td>-0.115370</td>\n",
              "      <td>0.088894</td>\n",
              "      <td>-0.298290</td>\n",
              "      <td>-0.207680</td>\n",
              "      <td>0.691100</td>\n",
              "      <td>0.068548</td>\n",
              "      <td>-0.50814</td>\n",
              "      <td>-0.97722</td>\n",
              "      <td>0.035782</td>\n",
              "      <td>-0.540530</td>\n",
              "      <td>...</td>\n",
              "      <td>0.197520</td>\n",
              "      <td>-0.254360</td>\n",
              "      <td>0.166160</td>\n",
              "      <td>-0.181350</td>\n",
              "      <td>0.163740</td>\n",
              "      <td>-0.161940</td>\n",
              "      <td>-0.320130</td>\n",
              "      <td>-0.331110</td>\n",
              "      <td>0.536620</td>\n",
              "      <td>0.53301</td>\n",
              "      <td>-0.321660</td>\n",
              "      <td>-0.643620</td>\n",
              "      <td>1.05760</td>\n",
              "      <td>-0.180190</td>\n",
              "      <td>-0.409320</td>\n",
              "      <td>-0.160840</td>\n",
              "      <td>-1.009400</td>\n",
              "      <td>0.229720</td>\n",
              "      <td>0.543100</td>\n",
              "      <td>0.058910</td>\n",
              "      <td>1.75020</td>\n",
              "      <td>-0.197530</td>\n",
              "      <td>-0.054010</td>\n",
              "      <td>0.016083</td>\n",
              "      <td>-0.555230</td>\n",
              "      <td>-0.202310</td>\n",
              "      <td>-0.326130</td>\n",
              "      <td>-0.38783</td>\n",
              "      <td>0.614280</td>\n",
              "      <td>-0.112160</td>\n",
              "      <td>0.423190</td>\n",
              "      <td>-0.447290</td>\n",
              "      <td>-0.35638</td>\n",
              "      <td>-0.326980</td>\n",
              "      <td>-0.126620</td>\n",
              "      <td>-0.288580</td>\n",
              "      <td>0.080920</td>\n",
              "      <td>0.144930</td>\n",
              "      <td>0.052563</td>\n",
              "      <td>0.750070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>'s</th>\n",
              "      <td>-0.005961</td>\n",
              "      <td>0.451480</td>\n",
              "      <td>0.004549</td>\n",
              "      <td>0.020727</td>\n",
              "      <td>0.538770</td>\n",
              "      <td>0.49453</td>\n",
              "      <td>-0.353690</td>\n",
              "      <td>-0.056286</td>\n",
              "      <td>0.057851</td>\n",
              "      <td>-0.204500</td>\n",
              "      <td>-0.306400</td>\n",
              "      <td>0.37904</td>\n",
              "      <td>0.263880</td>\n",
              "      <td>0.360360</td>\n",
              "      <td>0.687150</td>\n",
              "      <td>0.116290</td>\n",
              "      <td>-0.570050</td>\n",
              "      <td>0.084364</td>\n",
              "      <td>-0.671840</td>\n",
              "      <td>0.217060</td>\n",
              "      <td>0.600840</td>\n",
              "      <td>3.1461</td>\n",
              "      <td>0.218390</td>\n",
              "      <td>0.188300</td>\n",
              "      <td>-0.011606</td>\n",
              "      <td>0.702800</td>\n",
              "      <td>-0.054734</td>\n",
              "      <td>-0.576020</td>\n",
              "      <td>-0.505610</td>\n",
              "      <td>0.132750</td>\n",
              "      <td>-0.373490</td>\n",
              "      <td>-0.223150</td>\n",
              "      <td>0.189340</td>\n",
              "      <td>-0.388350</td>\n",
              "      <td>-0.446160</td>\n",
              "      <td>0.250780</td>\n",
              "      <td>-0.24067</td>\n",
              "      <td>-0.27303</td>\n",
              "      <td>-0.635870</td>\n",
              "      <td>0.071991</td>\n",
              "      <td>...</td>\n",
              "      <td>0.664030</td>\n",
              "      <td>-0.196290</td>\n",
              "      <td>-0.066911</td>\n",
              "      <td>0.208120</td>\n",
              "      <td>0.293660</td>\n",
              "      <td>0.330200</td>\n",
              "      <td>-0.219090</td>\n",
              "      <td>-0.279870</td>\n",
              "      <td>0.110260</td>\n",
              "      <td>0.47563</td>\n",
              "      <td>0.497420</td>\n",
              "      <td>0.071567</td>\n",
              "      <td>0.19419</td>\n",
              "      <td>-0.136880</td>\n",
              "      <td>-0.069569</td>\n",
              "      <td>0.497010</td>\n",
              "      <td>-0.843270</td>\n",
              "      <td>-0.447570</td>\n",
              "      <td>0.502530</td>\n",
              "      <td>0.605960</td>\n",
              "      <td>0.86543</td>\n",
              "      <td>0.475590</td>\n",
              "      <td>-0.009351</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>-0.599520</td>\n",
              "      <td>-0.123170</td>\n",
              "      <td>-0.303610</td>\n",
              "      <td>0.17132</td>\n",
              "      <td>0.823690</td>\n",
              "      <td>0.267900</td>\n",
              "      <td>0.334550</td>\n",
              "      <td>0.355210</td>\n",
              "      <td>-0.56247</td>\n",
              "      <td>0.372890</td>\n",
              "      <td>0.515540</td>\n",
              "      <td>-0.003640</td>\n",
              "      <td>0.076358</td>\n",
              "      <td>0.399470</td>\n",
              "      <td>0.296210</td>\n",
              "      <td>0.053627</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 200 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          1         2         3    ...       198       199       200\n",
              "the -0.071549  0.093459  0.023738  ...  0.336170  0.030591  0.255770\n",
              ",    0.176510  0.292080 -0.002077  ... -0.207740 -0.231890 -0.108140\n",
              ".    0.122890  0.580370 -0.069635  ... -0.039174 -0.162360 -0.096652\n",
              "of   0.052924  0.254270  0.313530  ... -0.086254 -0.419170  0.464960\n",
              "to   0.573460  0.541700 -0.234770  ...  0.544180 -0.230690  0.349470\n",
              "and  0.203270  0.473480  0.050877  ... -0.213580 -0.622490  0.143860\n",
              "in  -0.102720  0.304100 -0.135770  ...  0.369510  0.190390 -0.122660\n",
              "a    0.241690 -0.345340 -0.223070  ...  0.278010 -0.101710 -0.071521\n",
              "\"    0.001032  0.312010 -0.597680  ...  0.144930  0.052563  0.750070\n",
              "'s  -0.005961  0.451480  0.004549  ...  0.399470  0.296210  0.053627\n",
              "\n",
              "[10 rows x 200 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPbNIt2WD7Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for moving 'sos' token at index 0 and 'eos' token at index 1\n",
        " \n",
        "sos_index = word2idx['sos']\n",
        "eos_index = word2idx['eos']\n",
        "sos_swap_word = words[0]\n",
        "eos_swap_word = words[1]\n",
        " \n",
        "words[0], words[sos_index] = words[sos_index], words[0]\n",
        "words[1], words[eos_index] = words[eos_index], words[1]\n",
        "word2idx[sos_swap_word], word2idx['sos'] = word2idx['sos'], word2idx[sos_swap_word]\n",
        "word2idx[eos_swap_word], word2idx['eos'] = word2idx['eos'], word2idx[eos_swap_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8cjzqomEBoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort Word2idx\n",
        "word2idx = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaUn80e2BZXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rZDxdLL5klY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uH_VuWD5lvI",
        "colab_type": "code",
        "outputId": "2dd5323c-bfd7-4878-ee70-71fe50bda70c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhDp726q5pWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        if(name == 'eng'):\n",
        "          self.word2index = { k : v for k , v in sorted(word2idx.items(), key=operator.itemgetter(1))}\n",
        "          self.word2index[\"unk\"] = 400000 \n",
        "          self.word2count = { word : 1 for word in words }\n",
        "          self.index2word = { i : word for word, i in word2idx.items() }\n",
        "          self.n_words = 400001\n",
        "        else:\n",
        "          self.word2index = {}\n",
        "          self.word2count = {}\n",
        "          self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "          self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1tAZFIx5sAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLMVraBNFfmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 100\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH \n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br-e3-7E6EoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepareData(lang1, lang2, source_arr, target_arr):\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "    pairs = [];\n",
        "    for i in range(0,len(target_arr)):\n",
        "      pairs.append([normalizeString(source_arr[i]), normalizeString(target_arr[i])])\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rJpzUK52MeJ",
        "colab_type": "code",
        "outputId": "9dcae2a4-95a9-403b-fe54-35e00c64f4c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "input_lang, output_lang, pairs_train = prepareData('eng', 'vi', source_sent, target_sent)\n",
        "print(random.choice(pairs_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 133267 sentence pairs\n",
            "Trimmed to 132670 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 402460\n",
            "vi 14127\n",
            "['and here look the above question is put bluntly quot are video games art ? no . video games aren apos t art because they are quite thoroughly something else code . quot ', 'hay xem nhung cau hoi tren ay uoc hoi rat thang thung quot video game la nghe thuat u ? khong chung khong phai la nghe thuat vi chung hoan toan la thu gi o rat khac oan ma . quot ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upwI_rayXo4e",
        "colab_type": "code",
        "outputId": "dff2d952-74c3-4555-a191-c0c2b15ed4c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# create a new tensor with all embeddings for english using glove dictionary\n",
        "matrix_len = input_lang.n_words\n",
        " \n",
        "weights_matrix = np.zeros((matrix_len, 200))\n",
        "words_found = 0\n",
        "print(input_lang.n_words)\n",
        "print(len(input_lang.word2index))\n",
        "for i, word in enumerate(input_lang.word2index):\n",
        "    try: \n",
        "        weights_matrix[i-2] = glove[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i-2] = np.random.normal(scale=0.6, size=(200, ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "402460\n",
            "402459\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLyqU3WF700M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.input_size = input_size\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, bidirectional = True)\n",
        "\n",
        "    def forward(self, input, hidden, cell_state):\n",
        "        # print(\"forward running\")\n",
        "        # print(self.embedding(input).size())\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        # output, (hidden, cell_state) = self.lstm(output)\n",
        "        output, (hidden, cell_state) = self.lstm(output, (hidden, cell_state))\n",
        "        # print(output.size())\n",
        "        return output, hidden, cell_state\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(2, 1, self.hidden_size, device = device)\n",
        "    def initcellstate(self):\n",
        "      return torch.zeros(2, 1, self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ9UdlQE759q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# THIS CLASS ISN'T USED IN EVALUATION\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size,bidirectional =True)\n",
        "        self.out = nn.Linear(hidden_size*2, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden,cell_state):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden,cell_state = self.lstm(output, (hidden,cell_state))\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden,cell_state\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "    def initcellstate(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PotAA8uQ7_aI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 100\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size,bidirectional =True)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, cell_state, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        # print(attn_weights.unsqueeze(0).size(), encoder_outputs.unsqueeze(0).size())\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "        # print(embedded[0].size(),attn_applied[0].size())\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, (hidden, cell_state) = self.lstm(output, (hidden, cell_state))\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, cell_state, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(2, 1, self.hidden_size, device = device)\n",
        "    def initcellstate(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device = device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjVvbet88DNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    arr = [];\n",
        "    for word in sentence.split(' '):\n",
        "        try:\n",
        "          arr.append(lang.word2index[word])\n",
        "        except:\n",
        "          arr.append(lang.word2index[\"unk\"])\n",
        "    return arr\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZThP3EA8HRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_cell = encoder.initcellstate()\n",
        "    # print(\"setting optimizers to zero grad\")\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    # print(\"INPUT SIZE: \", input_tensor.size())\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "      # print(\"starting encoder for \" , ei)\n",
        "      encoder_output, encoder_hidden, encoder_cell = encoder(\n",
        "          input_tensor[ei], encoder_hidden, encoder_cell)\n",
        "      # print(encoder_output)\n",
        "      encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "    decoder_cell = encoder_cell\n",
        "    for di in range(target_length):\n",
        "      # print(\"decoder for di : \", di)\n",
        "      decoder_output, decoder_hidden, decoder_cell, decoder_attention = decoder(\n",
        "          decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "      if decoder_input.item() == EOS_token:\n",
        "          break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn2Gamqd8JiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdSWSxdv8NF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, pairs, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    # print(\"train Iter optimizers set\")\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    # print(\"training pairs for this iteration have been assigned\")\n",
        "    # print(\"training pairs size\")\n",
        "    # print(len(training_pairs))\n",
        "    # print(len(training_pairs[0]))\n",
        "    # print(len(training_pairs[0][0]))\n",
        "    # print(len(training_pairs[0][0]))\n",
        "    # print(training_pairs[0][0])\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        # print(iter , \" : printing iter-1 th training pair\")\n",
        "        # print(training_pair)\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        # if (iter % 100 == 0):\n",
        "        #     print(input_tensor)\n",
        "        # print(iter , \" : started training with above tensors\")\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        # print(iter,\" : current iter ended\");\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SDY5Z2G8g5L",
        "colab_type": "code",
        "outputId": "5f2dfbf4-c7ab-4009-dd82-14bf38eb42d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        }
      },
      "source": [
        "hidden_size = 200\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "print(\"Encoder initialization done\")\n",
        "# attn_decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "print(\"Decoder initialization done\")\n",
        "\n",
        "# trainIters(encoder1, attn_decoder1, 75000, print_every=5000)\n",
        "trainIters(encoder1, attn_decoder1, 20000, pairs_train, print_every=250)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder initialization done\n",
            "Decoder initialization done\n",
            "0m 59s (- 78m 13s) (250 1%) 3.4760\n",
            "1m 57s (- 76m 33s) (500 2%) 3.2649\n",
            "2m 56s (- 75m 19s) (750 3%) 3.1060\n",
            "3m 51s (- 73m 20s) (1000 5%) 3.2135\n",
            "4m 51s (- 72m 45s) (1250 6%) 3.2636\n",
            "5m 50s (- 72m 5s) (1500 7%) 3.8029\n",
            "6m 54s (- 72m 3s) (1750 8%) 4.5135\n",
            "7m 57s (- 71m 41s) (2000 10%) 4.5617\n",
            "9m 2s (- 71m 23s) (2250 11%) 4.6799\n",
            "10m 6s (- 70m 44s) (2500 12%) 4.7671\n",
            "11m 9s (- 69m 59s) (2750 13%) 4.8328\n",
            "12m 8s (- 68m 48s) (3000 15%) 4.5433\n",
            "13m 14s (- 68m 12s) (3250 16%) 4.6431\n",
            "14m 20s (- 67m 34s) (3500 17%) 4.8497\n",
            "15m 26s (- 66m 55s) (3750 18%) 4.7658\n",
            "16m 30s (- 66m 2s) (4000 20%) 4.6531\n",
            "17m 38s (- 65m 22s) (4250 21%) 4.8439\n",
            "18m 42s (- 64m 26s) (4500 22%) 4.6576\n",
            "19m 43s (- 63m 19s) (4750 23%) 4.6381\n",
            "20m 46s (- 62m 20s) (5000 25%) 4.9160\n",
            "21m 52s (- 61m 27s) (5250 26%) 4.7099\n",
            "22m 57s (- 60m 32s) (5500 27%) 4.4533\n",
            "24m 1s (- 59m 32s) (5750 28%) 4.8758\n",
            "25m 3s (- 58m 27s) (6000 30%) 4.6285\n",
            "26m 9s (- 57m 32s) (6250 31%) 4.5681\n",
            "27m 11s (- 56m 27s) (6500 32%) 4.7195\n",
            "28m 17s (- 55m 31s) (6750 33%) 4.7346\n",
            "29m 21s (- 54m 31s) (7000 35%) 4.6122\n",
            "30m 24s (- 53m 28s) (7250 36%) 4.5804\n",
            "31m 33s (- 52m 36s) (7500 37%) 4.7563\n",
            "32m 42s (- 51m 41s) (7750 38%) 4.7157\n",
            "33m 46s (- 50m 40s) (8000 40%) 4.6177\n",
            "34m 48s (- 49m 33s) (8250 41%) 4.5070\n",
            "35m 46s (- 48m 23s) (8500 42%) 4.4930\n",
            "36m 51s (- 47m 23s) (8750 43%) 4.6424\n",
            "37m 55s (- 46m 21s) (9000 45%) 4.7816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-J0HQm5hxK8",
        "colab_type": "code",
        "outputId": "6d17456d-aa89-4461-8b1a-645e6fc4a6fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "input_lang_test, output_lang_test, pairs_test = prepareData('eng', 'vi', test_source_sent, test_target_sent)\n",
        "print(random.choice(pairs_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 1268 sentence pairs\n",
            "Trimmed to 1265 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 400032\n",
            "vi 1104\n",
            "['i didn apos t know he was abusing me .', 'toi khong biet anh ta ang bao hanh toi .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9We9ZU8bre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        encoder_cell = encoder.initcellstate()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size*2, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden,encoder_cell = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden,encoder_cell)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_cell = encoder_cell\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden,decoder_cell, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1,:ei + 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy1dYNPb8eHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n = 5):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs_test)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DXfF12H8rNL",
        "colab_type": "code",
        "outputId": "41cf4ef4-9f38-45ff-9674-6a5ba4df6231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Ronak is the best.\n",
            "= nhin qua cong truong ap a co ve ay nhung nguoi khoe manh.\n",
            "< toi la toi la <EOS>\n",
            "\n",
            "> under the taliban girls who went to school numbered in the hundreds remember it was illegal .\n",
            "= duoi taliban nhung co gai en truong ca hang tram nguoi nho rang o la bat hop phap .\n",
            "< toi la toi la la . la <EOS>\n",
            "\n",
            "> and making your ideas accessible is not the same as dumbing it down .\n",
            "= lam y tuong minh de hieu khong phai la ha thap chung .\n",
            "< toi toi la la . la toi la <EOS>\n",
            "\n",
            "> the dyed black hands are the father while the blue and red hands are his sons .\n",
            "=  oi tay nhuom en la nguoi cha nhung ban tay xanh va o la con trai ong ay .\n",
            "< toi toi la toi la <EOS>\n",
            "\n",
            "> how can we lend and borrow more things without knocking on each other apos s doors at a bad time ?\n",
            "= lam the nao chung ta cho muon va muon nhieu thu hon ma khong go cua nha cua nhau vao thoi iem xau ?\n",
            "< toi toi la la . la la <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKKVX8GNBzeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in range(50):\n",
        "#   sent = random.choice(pairs_test)[0]\n",
        "#   output_words, attentions = evaluate(\n",
        "#       encoder1, attn_decoder1, sent)\n",
        "#   print(attentions.size(), sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc_VdBg6EzfT",
        "colab_type": "code",
        "outputId": "b5c4fc55-5c0c-46e1-9e3b-63d78fc52565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(random.choice(pairs_test)[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = to him there was greater risk in not educating his children .\n",
            "output = va toi la <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAADnCAYAAADxaEqtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZxkdX3u8c8zK9vMADOAymhAQbzgzogYMSoIgkLQAILhihoUo3KzasQkohLiFaPxFXdRNokKiAsjgiggEhCRGSBswr0jiqxyG5BNEGf6uX/8TvVU9/R0VVed7qrqed7zOq+uc+rUr789XV3fc36rbBMREQEwq9cBRERE/0hSiIiIEUkKERExIkkhIiJGJClERMSIOb0OICJiUO27774eGhpqed7KlSsvsL3vNITUtSSFiIgODQ0NsWLFipbnSVoyDeHUIkkhIqILM22sV5JCRESHDKwZHu51GLVKUoiI6JgxuVOIiAgAw/DMyglJChER3UibQkREAKVNYThJISIiGnKnEBERQEkI6X0UEREjcqcQEREj0iU1IiKARkNzr6OoV5JCREQXUn0UEbWQtOU4hx+2/YdpDyY6k4bmiKjR1cBTgQcAAZsD90j6DfB22yt7GVy0ZmbenUIW2YnonR8Cr7G9xPZiYD/gXOBdwOd6Glm0bdhuuQ2SJIWI3tnd9gWNHds/AF5i+6fA/N6FFZNhu+U2SFJ9FNE7d0t6H3BGtX8o8BtJs4GZVVE9Y828WVJzpxDRO38OLAW+U21Pq47NBt7Qw7iiTa5mSW21DZLcKUT0iO0h4H+t5+lV0xlLdG44vY8iog6Sngm8B9iOpr9F23v2KqaYnMySGhF1+gbwBeDLwJoexxIdGrSG5FaSFCJ6Z7Xtz/c6iOjCAHY5bSVJIaJ3vivpXcC3gd83Dtq+v3chjSZpPnAQ61ZxHdermPpN7hQioi5vrr6+t+mYgaf3IJb1OQd4EFhJU+KKwsCaJIWIqIPt7XsdQxuW2t6310H0s9wpxAarGlR1gu339DqWQSZpT9sXS/qz8Z63/a3pjmkCP5H0HNvX9zqQfpWkEBss22sk7dHrOGaAlwMXAweM85yBfkoKewBvkfRLSvWRANt+bm/DGk2SKG0z77f98+n6vk5DcwTXSFpO6U75aONgn13d9jXbH6weHmf7l83PSeq3KqX9eh1Am/YBXgS8Dfj76fzGM+1OIdNcxGRtBNwH7Em50j0A2L+nEQ2ub45z7Oxpj2ICtm+jTO+9Z/X4d/Tn58aRlIRwgKRpvdjNhHixQbP91l7HMOgkPQvYBVg0pl1hISXp9g1JHwSWATsBpwBzgf8EXtrLuJpJWgLsYvt8SQcAr2OakmvpfTSzprnox4wffUzSMyVdJOmGav+5kv6513ENmJ0od1ebs/Zu6wDghcDbexjXeF4P/ClVVaHtu4AFPY1oXW8Cvl49PoVyxzBtMiFebOi+ROlX/0UA29dJ+hpwfE+jGiC2zwHOkfQS21f0Op4WnrBtSQaQtGmvAxrHXwD7Ati+StKTJT3V9u1T/p0HsHqolSSFmKxNbP+sdPYYsbpXwQy4ayS9m1KVNFJtZPsvehfSOs6S9EVgc0lvp3wAf6nHMY2QtDnwGdt3Nh1+D7AEmPKkkOU4I2BI0jMofw9IOhi4u7chDazTgScBrwZ+TFlb4eGeRtSk6uZ5JqV+/puUaq9jbX+6p4E1sf1b218cc+yHtq+ZrhjqWo5T0r6SbpG0StIx4zw/X9KZ1fNXStquOr63pJWSrq++7tn0mkuqMq+ttq1bxZE7hZisdwMnAs+SdCfwS+Dw3oY0sHawfYikA22fVlXD/Vevg2qoqo3Os/0cynrSfaW6c7nE9v+tEtjJlHmafgW8eboSQx13CtXA0M8CewN3AFdJWm77pqbTjgQesL2DpMOAEyir9Q0BB9i+S9KzgQuAbZted7jtFe3GkjuFmCzbfhWwFfAs23uQ91Gn/lB9/W31x7wIaHklN82ulvSiXgexHn9NSQAAbwSeC2wP/B3wqekIwDZrhodbbm3YDVhl+1bbT1CWaD1wzDkHAqdVj88G9pIk29dUHQAAbgQ2riYy7Ej+mPuIpG0knSTp/Gp/Z0lH9jquMb4JYPtR242qjr7qWz9ATpS0BfDPwHLgJuBjvQ1pHS8GrpD0C0nXVVUU1/U6qMpq243Euj/wFdv32b4QmLYGcbfxD1giaUXTdtSYYrZldBvIHYy+2h91ju3VlIkKF4855yDgatvNkxeeUlUdfUBjGgPHk+qj/nIqpUvdP1X7/4dSp3tSrwJqGKS+9YPC9perh5fSXzOjNnt1rwOYwLCkJwMPAHsB/9r03MbTFkR7tUdDtpdNZRySdqFUKe3TdPhw23dKWkC5oHsT8JWJysmdQn9ZYvssYBhGrgb6ZUWuQepbPxAkfaTqPdPY30JSX3TtlbSlpC0pDd/jbf3gWGAFpQppue0bASS9HLh1OgJo9D6qYUTznZSR4w1Lq2PjnlON2l5EmV0ASUspcz8dYfsXI/FVvbKqu/qvUaqpJpQ7hf7yqKTFrO3ZszvlFrHnBqxv/aDYz/Y/NnZsPyDpNZTqpF5bSXkfCnga5WpclIuCX1Pq7nvK9rmS/ghYYPuBpqdWUBpgpyuOOoq5CtixmvvqTuAw4M/HnLOcsgbHFcDBwMVVZ4DNge8Bx9i+vHFylTg2tz0kaS7lou7CVoEkKfSXv6P84p8h6XJKY+7BvQ1pHYPQt35QzJY0v1H/K2ljoOMGwjo11nqQ9CXg27bPq/b3o0wj0S+2BN5dVZ1AaWj9nO3fTMt3rxqauy/GqyUdTek5NBs42faNko4DVtheTqlGPl3SKuB+SuIAOBrYAThW0rHVsX0oo9AvqBLCbEpCaDnGJEmhT1Rd0l5ebTtRrspuaWpI6xenAzdT6pqPo3RHnbapimeYrwIXSTql2n8ra3uX9IvdbY9UD1bzC/VFY7ikl1KqRE5lbT35rsCVkg5vvmqeKnUOXqsS73ljjh3b9Phx4JBxXnc8659RYNfJxpGk0CeqtQreaPuTlKudftXXfesHie0Tqp48e1WH/sX2Bb2MaRx3VXNb/We1fzhw1wTnT6dPAK8bMx5huaRvU6ZhefF0BJH1FGIdkhYyelHzThdev1zSZyg9jprXKri6w7g+RrmCeAz4PqUf99/a/s8JXzixsX3r76H/+tYPDNvnA+f3Oo4JvBH4IKURE0pPqTf2LpxRFo43QM32tVVvm2lRdTmdMZIUuiDpHcCHgcdh5J3RzcLrz6++Htd0zJS1Czqxj+1/kPR6Sg+NP6P8UXeTFBp96z9Aaf/YjNILpGOSjrR90phjH7W9zlD/mUTSw6x938yjTEv9qO2FvYtqtOoC5697Hcd6SNIWYxqZqXpNTVvPyhl2o5Ck0KX3AM+2PVRHYbZfWUc5TRq/39cC37D9YBtjVybU1Lf+x9TXt/4gSY/b/iqApM/SZ2MfmhuEJzo2GbZHrmarQUUHArt3HmV9JH0X1n8JbPtPpzGc9fkk8ANJ7wEad9O7Uvrqf3I6AjCpPorRfkFZiaoWkrYBPgI8xfZ+knYGXjL2KnoSzpV0M6X66J2StqLc1fRTjFBGYS6XNEyZAvm3tvttJPcVlDEZrY51xKW18jsqi9r0wx3Sx3sdQCu2T5R0F/AvlN5wpowKP972d6cpiFp6H/WTJIXuvB/4iaQrKYuaA2D7rzos71RqHNFs+5iqXeHBqiH7UdadT6VnMVa3+Q1vA74DXA58WNKWXbTN1EbSkyjTC2ws6QWUXmFQRnJv0mXZzSPDZ1FWOOsqadfF9o97HUM7bJ8LnNuz78/Mmzo7SaE7XwQuBq6nGoXcpSW2z5L0fhjpu9ztiOanAK+S1FwdM+Ew9xbqjLF5gFTj62urrZu2mTq9GngLZYTpvzcdfxj4x/FeMAkHND1eTWn36Spp19W5QNJZtt8g6XrGqUay/dxu4qxDI8bq8Qm239f03A9s77P+V9cnSSGazbX9dzWWV+uI5qoq4hXAzpT+z/sBl9FdUqgtxsYAqX5m+zTgNEkH2f5mzWVPxXrXdXUuaDQu719jbHXbsenx3sD7mva3mq4g0qYQzc6vZjv8LqOrjzqt9qh7RPPBwPOAa2y/tWoP6Kbn0VTEiKRDgO/bfrjqE/9CSp/9aVsopQ0XSfp34E+q/R8Dx9medEKU9GkmbsTttPoRaupcYPvu6uttXcQy1Sb6NJ6mT2qnS2qM0uiv/f6mYx1Xe9i+uprMq64RzY/bHpa0uhpLcS+jJ92alCkcdf0B29+QtAfwKuDfgC8wTYOP2nQScAPwhmr/TZS2lT9b7yvWr7HgyUspd3FnVvuHUBpKu1Fr54Kq3eMEylgUVZv7pNvsJlU7zyxGt/mIaZol1U6X1IFVXSU3Fgv5me17uy1ziqo/dgO2o/xuXigJ251W91xVTZb1JUr9/SOUHjMdmcJR1402idcCJ9r+Xr/MFtrkGbYPatr/sKRrOymoqpJC0juBParZcJH0BbocHT4FnQs+RlnVqx+nMrmbte089zC6zeee6QoivY8GkKQ3UK4+L6FcRXxa0nttd7Q4jKQ9bV88pvfICNvf6rDc04FnANey9oPSdN4GsJBy9XkJpdFxoe1uF0ipddR15U6VxeH3Bk5QWTWq68FHkv6YtQkWoJsE+5ikPWxfVpX9UsrVeDe2oPyOGtWNm1XHJm289+SYaqOO3pPAb/o0IUzFuJ7Jx0DaFAbVPwEvatwdVLfUF9L5imEvp/Q6avQeabwrGr1oOv0DXAbs7Pq6M5wEvAz4NCXZXCPpUtv/0UWZjVHXH66+Nn7mTkddQ6mS2Rf4uO3fqiyc8t4uypuKBPtOSoPzomr/Aco0xt34KGW5y0so/49/Anyow7Ka35PN75+O3pNNyWWFpDMp3YWb2806fY/XSmVm2Wfa/u+mY08D1rhaS2CqpffRYJo1prroPrq4ErX9werhOykDr7Zj7f9lN++QG4AnUW6Lu2b7R5IupVSbvRL4S8ogn26Swrms7T5K9fghSc+3PanqFEkLbT9EGb18SXVsS8qHT9sLja9H3Qn255SqlGdQ1hR4kDKFdDd3XqdSEtbfUJLBByi//0mbgvdk8wXP7xi9mlc3Fz51Ww18S9JzbTfuXL9M6S489Umh/UV0BsaGkhTOl3QB8PVq/1DGTFHboe8Av6UMsW805k36HdI0pcAC4CZJP2P0VVlHUwpIuoiyVu0VlLrqF7n7tpRdKR+4yymJYX/KB+M7JH3D9mSmVf6apAOAIUr3yeb6jm7HKdSaYIFzWPu7ruvD5nOU8S0b215ezSn1Tda2fXWilvdko7uspNOAv7b922p/C8rspH3B9h9UZkV9A2Ut4qcBW9nu9qJiMkFM27eaDhtKUjBloNke1f6J1DPHzFLb+9ZQzscpH4gnMHoBk8axTl1H+RB/NuXK9reSrrDdTV34UuCFth+BkbEQ36NUfaxkEgvP296/KuMm28/uIqbxLKHGBEt9v+tmL7b9QknXwMjKa/O6LLPuOJ/bSAgwEuMLaiy/Dl+m/E2fAhxRfZ02w2uSFAbR3tVox5FbXkkfZvRgl078RNJzbF/fTSGNKQUkzR07vUBVZ9ppuX9blbGAMir3FMrVczere21N04csZSrtbWw/JqnTyeFWSnqR7au6iGusD9VYFtT0ux7jD1U338ZAwK3ofmR83XHOUtNMpFX1Xl99bti+WcUzKauRvWz6vnfaFAZK1eXvXcDTVRYzaVhAmWOn03IbQ//nAG+VdCvlg7LRh3tSUwBMYZxHU/5AdqVUz5xM9wvifJWystU51f4BlGqgTem8j/2LgcMl3Ubp0dTR/2OzKZi7Zw/gLZJ+SRe/6zE+RVmnYGtJ/0oZBNjR+sx1vyebfAK4QtI3qv1DgH/tsKz1kvQk2910Iz2JcsdwvcdMpT3VZlpS0Ez7gZpVPUW2AP43o2eefLibydZUFgtfr8mOAp3CON9DSQIrG33h6yBpGWXgFcDl3dbfru//s5PRtJIus72HRq9VAF0OuqozxjHlPouy8pqAizrt/ln3e3JM2TuztnfZxba7HWA33vf4nu3XdvH6TSjtRwfZbrk4fV2232knf+gLJ7Y87y17vmKl7WXTEFLXZnRSiIiYStvvtJM/+Lkvtjzvra965cAkhRldfRQRMZVmYpvCtC1Z109UJrHr6zIHIcapKDMx9m+ZG2qMrXh4uOU2SDbIpABMxRun7jIHIcapKDMx9m+ZG2qME2pMijfRNkhSfRQR0SkbDw/Yp34LMyopbLZwkRdvvXXL87bcaiv+aIcd2/pN3vHLX7b1vaVZzJ49p2WZ8+e3t4Lj3LkbsckmC9t8t7V32ty589lkkwUtT169uv2OSrNnz2HevI1bljl7dntvtblz57Pxxq1jBLDbuy2fM2ceG220aW0xzpu3EZtuuqitGDdb2F5npwULt2CbJz+1rTLv+3/t9dyUZjFnztw2fu65bZU3e/Zc5s/fpK0YZ81qrxJi7px5bLzxZi3LVJtrQrT7Hgd47LFHhmx3vRjPTGtTmFFJYfHWW/O+j3czrc+6jjniTbWWt8MOu9ZaHsCaNd0uZzDaAw/8ptbyABYuXFx7matXP1FreQsXLqm1PIDd99yr9jK/8oWP1lreFlt0NN3ShDbZeEGt5c2Z2+1A73Vdd90lXS8glDWaIyJilCSFiIgobLxmsHoXtZKkEBHRhdwpRETEiBmWE5IUIiI6lYbmiIhYK9NcRETEWmZ4zXDLrR2S9pV0i6RVko4Z5/n5ks6snr9S0nbV8b0lrZR0ffV1z6bX7FodXyXpU2pjwEfPk4Kkj0p6d9P+hyT9s6SLJF1d/UAH9jLGiIj1cbVO80RbK9ViS58F9gN2Bt5YTVne7EjgAds7AJ9k7aqMQ8ABtp8DvBk4vek1nwfeDuxYbS1X5et5UgDOpKyv2vAG4DTg9bZfSFlw/hPry3CSjpK0QtKKRx56cOqjjYioNGZJ7TYpALsBq2zfavsJ4Axg7MXwgZTPRoCzgb0kyfY1tu+qjt8IbFzdVTwZWGj7py5BfIXRy/2Oq+dJwfY1lJWnniLpecADwD3AR6pVyC4EtgW2Wc/rT7S9zPayzRYumra4IyKAdmfEW9K4eK22sRP3bQvc3rR/R3Vs3HOqRbMeBMZOFXAQcLXt31fn39GizHX0S0PzNyhLET6JcudwOLAVsKvtP0j6FbBR78KLiBhfm1NwDU31IjuSdqFUKe3TTTk9v1OonElZcPtgSoJYBNxbJYRXAhMuNRgR0Ss1VR/dCTy1aX9pdWzccyTNoXxO3lftL6Ws932E7V80nb+0RZnr6IukYPtGyiL1d9q+m7I4/LJqMfIjgJt7GV9ExLhshoeHW25tuArYUdL2kuZRLpKXjzlnOaUhGcoF9MW2LWlz4HvAMbYvXxua7wYekrR71SZ7BHBOq0D6pfqIquW88XgIeEkPw4mIaKmuwWu2V0s6GrgAmA2cbPtGSccBK2wvB04CTpe0CrifkjgAjgZ2AI6VdGx1bB/b9wLvAk4FNgbOr7YJ9U1SiIgYOKa2RXZsnwecN+bYsU2PHwcOGed1xwPHr6fMFcCzJxNHkkJERDdm2IjmJIWIiI613ZA8MJIUIiK6MJw1mvvXXbf9muP+8t2tT5yEg9/4V7WWd+VlP6y1PIAdd6x3ic/NNru99UmT9Otf/7z2Mhctqnf5zJe9puUMAJN24be/VXuZW2755NrLrNtrDvvzWsv79c9/XWt5ANddd0nXZbjGNoV+MaOSQkTEdEv1UUREjEhSiIiIShqaIyKiYQYuspOkEBHRIQNek6QQERGV3ClERETR/iyoA6Ons6RK2lzSu1qc8xRJZ09XTBERk+Fht9wGSa+nzt6cMovfetm+y/bB0xRPRMSk1LSeQt/odfXRR4FnSLoWaAz13Y/SfnO87TMlbQeca3tSM/1FREy1uqbO7ie9TgrHAM+2/XxJBwF/CTwPWAJcJenSVgVUa50eBTBr1uypjDUiYjQbt7eIzsDodfVRsz2Ar9teY/s3wI+BF7V6ke0TbS+zvSxJISKmm4dbb4Ok13cKEREDbaZVH/X6TuFhytrMAP8FHCpptqStgD8BftazyCIiWnEammtl+z5Jl0u6gbJ26HXAf1Pab/7B9j1VQ3NERN9JQ/MUsD124vX3jnn+V0xyjdGIiOlhhtcMWKNBCz1PChERAysT4kVExChJChER0TDDcsLMSgqbbLKQF7xg71rLPO+cU2stb86cubWWB3DXXatqLe+hh4ZqLQ9g6dKdai/zpz89p9bybp+CdaSf/ozn1V7mpptuXmt5i6dgzefzzvx6reXdeONltZYHcNYZH+u6jDQ0R0TEWmbgJrxrJUkhIqJjZniGTXORpBAR0YVUH0VExFpJChERASUfpE0hIiJGzLAbhSSFiIjODd6Ed630XVKQ9IjtzXodR0RESya9jyIiojAzr02h1+sprJekzSRdJOlqSddLOrDXMUVEjFXXegqS9pV0i6RVko4Z5/n5ks6snr+ysayApMWSfiTpEUmfGfOaS6oyr622rVvF0c93Co8Dr7f9kKQlwE8lLfeY/+HmNZo32ii1ThExnVxLS7Ok2cBngb2BOyhr1C+3fVPTaUcCD9jeQdJhwAnAoZTPyg9QlhgYb5mBw22vaDeWvr1TAAR8RNJ1wIXAtsA2Y09qXqN53ryNpjvGiNiQ1bfy2m7AKtu32n4COAMYWztyIHBa9fhsYC9Jsv2o7csoyaFr/XyncDiwFbCr7T9I+hWQT/2I6CvDa9r60F8iqflq/UTbJzbtbwvc3rR/B/DiMWWMnGN7taQHgcVAqxksT5G0BvgmcPzY2pax+jkpLALurRLCK4E/6nVAERHNJjFL6pDtZVMczngOt32npAWUpPAm4CsTvaCfq4++CiyTdD1wBHBzj+OJiBitvuqjO4GnNu0vrY6Ne46kOZQL5/smDM++s/r6MPA1SjXVhPruTqExRsH2EPCSHocTETGB2gavXQXsKGl7yof/YcDY9euXA28GrgAOBi6eqCqoShyb2x6SNBfYn9I+O6G+SwoREYOkjqRQtREcDVwAzAZOtn2jpOOAFbaXAycBp0taBdxPSRwAVG2uC4F5kl4H7APcBlxQJYTZlITwpVaxJClERHShrsFrts8Dzhtz7Nimx48Dh6zntdutp9hdJxtHkkJERIcyS2pERIySCfH62O9+9xDXXntRrWU+/enPr7W8RYu2qrU8gDvuuKXW8rbfvv7F5u+7b2xHiu4tWbK01vKe/vT6f+7777+n9jIXLlxca3mzZtf/MbBw4ZJay/vFvffWWl59MktqREQ0pPooIiKa5U4hIiKASY1oHhhJChERHTPOIjsREQFUbQq9DqJeSQoREV1I9VFERIyYaUmh61lSx1nu7eym546SdHO1/UzSHk3P7S/pGkn/LekmSe/oNpaIiOnUaGiuYznOftHRnYKkecBc249Wh9ZZ7k3S/sA7gD2qWfpeCHxH0m6U6V5PBHazfYek+cB21eu2sP1AZz9ORMQ0shleM7MaFSZ1pyDpf0j6BHAL8MwWp78PeG81BTa2r6YsJfduYAElId1XPfd7241huYdKukHS30uqf/hvRESd7NbbAGmZFCRtKumtki6jTLt6E/Bc29c0nfbVpuqjf6uO7QKsHFPcCmAX2/dT5ga/TdLXJR0uaRaA7S8A+wGbAJdKOlvSvo3nx4nvKEkrJK0YnmFdwyKi/7mNf4Okneqju4HrgLfZXt/qZ+tUH7Vi+22SngO8CngPsDfwluq524F/kXQ8JUGcTEkofzpOOSdSqqKYO3f+YP3vR8RAszfMhuaDKSsBfUvSsZLaXSv5Jtady3tX4MbGju3rbX+SkhAOaj6xanv4HPAp4Czg/W1+34iIaWLs4ZbbIGmZFGz/wPahwMuAB4FzJF0oabsWL/0YcIKkxQCSnk+5E/icpM0kvaLp3OdTVglC0j6SrgOOB34E7Gz7b2zfSEREn9lgex/Zvg/4D+A/qqv4NU1Pf1XSY9XjIduvsr1c0rbATyQZeBj4n7bvlrQA+AdJXwQeAx6lqjqiND4fYPu2rn6yiIhpMNPaMjvqkmr7Z02PXzHBeZ8HPj/O8YeB16znNWMbpyMi+lK5E0hSiIiIhgGrHmolSSEioguD1uW0lSSFiIguDFpDcitJChERHTPDw2tanzZAZlRSWL36iaF77rm1nV5LS4Chdsq8555b2/32bZfZo/LaLvOGGy6tvcweltd2mUNDd9Ra3iQNQpk9i3GHbc6ptbxKu2Ou1msmDl6bUUnBdltzJUlaYXtZnd+77jIHIcapKDMx9m+ZG2qMrSQpRETEiCSFiIioDN4sqK1sqEnhxAEocxBinIoyE2P/lrmhxjghM7MGr2mm3fpEREyXBQsWe9myV7c875JLvr5yuts6OrWh3ilERNRg8Ca8ayVJISKiC5n7KCIiRsy0O4VJrdEcERGj1bWeQrXs8C2SVkk6Zpzn50s6s3r+ysaaNpIWS/qRpEckfWbMa3aVdH31mk9JUqs4khQiIjplt7e1IGk28FnK8sM7A2+UtPOY044EHrC9A/BJ4ITq+OPAByjLGo/1eeDtwI7Vtm+rWJIUIiI6ZGDYa1pubdgNWGX7VttPAGcAB44550DgtOrx2cBekmT7UduXUZLDCElPBhba/qnL7cpXgNe1CiRJISKiY62rjqrqoyWSVjRtR40paFvg9qb9O6pj455jezVleeTFEwS3bVXORGWuIw3NERFdaLPNYGhQxinkTiEiogs1NTTfCTy1aX9pdWzccyTNARZR1rSfqMylLcpcR5JCRESHSjvycMutDVcBO0raXtI84DBg+ZhzlgNvrh4fDFzsCTKO7buBhyTtXvU6OgJoOQd5qo8iIjpmPNz94DXbqyUdDVwAzAZOtn2jpOOAFbaXAycBp0taBdxPSRwASPoVsBCYJ+l1wD62bwLeBZwKbAycX20TytxHEREd2nTTRd555z9ued6KFd/P3EcRERuCmXZhnaQQEdExZ+6jiIgoskZzRESMkqQQEREjhmvofdRPkhQiIjpmSJtCREQ0mFQfRUQEaWiOiErOsc0AAALfSURBVIgxkhQiIqKScQoREdEkvY8iIgJIm0JERIzS3hrMgyRJISKiCybVRxERUUn1UUREVJyG5oiIKBrLcc4kSQoREV1I9VFERIxIUoiIiEq6pEZERJPMkhoREUC5SRgeXtPrMGqVpBAR0TGnTSEiItZKUoiIiBFJChERMSKD1yIionC6pEZERMXAcO4UIiKiIdVHERFRSZfUiIhokqQQERFA1miOiIhRjGfYNBezeh1ARMQgcxv/2iFpX0m3SFol6Zhxnp8v6czq+Sslbdf03Pur47dIenXT8V9Jul7StZJWtBNH7hQiIrpQR/WRpNnAZ4G9gTuAqyQtt31T02lHAg/Y3kHSYcAJwKGSdgYOA3YBngJcKOmZthu3MK+0PdRuLLlTiIjogu2WWxt2A1bZvtX2E8AZwIFjzjkQOK16fDawlyRVx8+w/XvbvwRWVeV1JEkhIqJD5UN/uOUGLJG0omk7akxR2wK3N+3fUR0b9xzbq4EHgcUtXmvgB5JWjvM9x5Xqo4iILrR5JzBke9lUxzKOPWzfKWlr4IeSbrZ96UQvyJ1CREQXhoeHW25tuBN4atP+0urYuOdImgMsAu6b6LW2G1/vBb5NG9VKSQoREd1oTIo30dbaVcCOkraXNI/ScLx8zDnLgTdXjw8GLna5TVkOHFb1Ttoe2BH4maRNJS0AkLQpsA9wQ6tAUn0UEdExY7qf+8j2aklHAxcAs4GTbd8o6Thghe3lwEnA6ZJWAfdTEgfVeWcBNwGrgXfbXiNpG+DbpS2aOcDXbH+/VSyaaaPxIiKmy6xZs73RRpu0PO+xxx5Z2aM2hUnLnUJERBdm2oV1kkJERBeSFCIiomKGZ9jcR0kKEREdyiypERExWpJCREQU7c+COiiSFCIiupA1miMiYkSb01gMjCSFiIjOXQAsaeO8ttcz6LWMaI6IiBGZEC8iIkYkKURExIgkhYiIGJGkEBERI5IUIiJixP8HaRhjl7JNCDQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-6wIjIdpVNu",
        "colab_type": "text"
      },
      "source": [
        "1. BiLSTM\n",
        "2. GloVE embedding\n",
        "3. presentation\n",
        "4. BLUE Score and how error is calculated here\n",
        "5. Figure out how attention figures are made.`\n"
      ]
    }
  ]
}